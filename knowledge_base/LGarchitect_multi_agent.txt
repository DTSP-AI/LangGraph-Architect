##### Hierarchical Tool Swarm Orchestration via LangGraph #####

You are an AI architecture agent specialized in designing and executing hierarchical swarm systems using LangGraph and LangChain. Build modular, multi-agent workflows composed of the following:

Top-Level Supervisor: Routes user requests between distinct subgraphs (e.g., Research Team, Document Team).

Mid-Level Subgraph Supervisors: Each team has a supervisor node using structured LLM outputs to delegate tasks among internal worker nodes.

Specialized Worker Agents: Each agent executes a specific role using create_react_agent with toolsets like web scraping, Tavily search, document editing, and Python REPL execution.

Core Requirements:

Use the StateGraph abstraction with custom State schemas to carry shared memory across nodes.

All state transitions must explicitly route back to the relevant supervisor after task completion.

Chain logic must support dynamic branching (goto) and recursive streaming (graph.stream()).

Tool execution must be safe (guard local execution), modular, and scoped to agent capabilities.

Compose agents as nested LangGraph graphs to enable hierarchical delegation (e.g., super_graph → research_graph → web_scraper).

Persist memory using LangGraph-compatible state objects (MessagesState) and support recursion.

Base system on your stack: langchain==0.3.23, langgraph==0.3.31, OpenAI, PGVector, SQLAlchemy, Streamlit, and Python REPL.

Behavioral Expectation: Each node in the graph must act as a self-contained reasoning unit that:

Accepts shared state,

Executes task using its defined toolset,

Returns updated state,

Reports back to its supervisor.

Final Output: A functioning multi-agent LangGraph application capable of executing nested workflows, routing decisions via LLM-based supervisors, and dynamically resolving tool calls within a structured message-passing graph.


Example Notebook


Hierarchical Agent Teams
In our previous example (Agent Supervisor), we introduced the concept of a single supervisor node to route work between different worker nodes.

But what if the job for a single worker becomes too complex? What if the number of workers becomes too large?

For some applications, the system may be more effective if work is distributed hierarchically.

You can do this by composing different subgraphs and creating a top-level supervisor, along with mid-level supervisors.

To do this, let's build a simple research assistant! The graph will look something like the following:

diagram

This notebook is inspired by the paper AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation, by Wu, et. al. In the rest of this notebook, you will:

Define the agents' tools to access the web and write files
Define some utilities to help create the graph and agents
Create and define each team (web research + doc writing)
Compose everything together.
But before all of that, some setup:


from getpass import getpass
import os
import uuid


def _set_if_undefined(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass(f"Please provide your {var}")


_set_if_undefined("OPENAI_API_KEY")
_set_if_undefined("TAVILY_API_KEY")

# Optional, add tracing in LangSmith.
# This will help you visualize and debug the control flow
os.environ["LANGCHAIN_TRACING_V2"] = "false"
os.environ["LANGCHAIN_PROJECT"] = "Multi-agent Collaboration"


     
Create Tools
Each team will be composed of one or more agents each with one or more tools. Below, define all the tools to be used by your different teams.

We'll start with the research team.

Research team tools

The research team can use a search engine and url scraper to find information on the web. Feel free to add additional functionality below to boost the team performance!


from typing import Annotated, List, Tuple, Union

import matplotlib.pyplot as plt
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.tools import tool
from langsmith import trace

tavily_tool = TavilySearchResults(max_results=5)


@tool
def scrape_webpages(urls: List[str]) -> str:
    """Use requests and bs4 to scrape the provided web pages for detailed information."""
    loader = WebBaseLoader(urls)
    docs = loader.load()
    return "\n\n".join(
        [
            f'<Document name="{doc.metadata.get("title", "")}">\n{doc.page_content}\n</Document>'
            for doc in docs
        ]
    )


     
Document writing team tools

Next up, we will give some tools for the doc writing team to use. We define some bare-bones file-access tools below.

Note that this gives the agents access to your file-system, which can be unsafe. We also haven't optimized the tool descriptions for performance.


from pathlib import Path
from tempfile import TemporaryDirectory
from typing import Dict, Optional

from langchain_experimental.utilities import PythonREPL
from typing_extensions import TypedDict

_TEMP_DIRECTORY = TemporaryDirectory()
LOCAL_WORKING_DIRECTORY = Path(_TEMP_DIRECTORY.name)
print(LOCAL_WORKING_DIRECTORY)
WORKING_DIRECTORY = Path("./work_dir")


@tool
def create_outline(
    points: Annotated[List[str], "List of main points or sections."],
    file_name: Annotated[str, "File path to save the outline."],
) -> Annotated[str, "Path of the saved outline file."]:
    """Create and save an outline."""
    with (WORKING_DIRECTORY / file_name).open("w") as file:
        for i, point in enumerate(points):
            file.write(f"{i + 1}. {point}\n")
    return f"Outline saved to {file_name}"


@tool
def read_document(
    file_name: Annotated[str, "File path to save the document."],
    start: Annotated[Optional[int], "The start line. Default is 0"] = None,
    end: Annotated[Optional[int], "The end line. Default is None"] = None,
) -> str:
    """Read the specified document."""
    with (WORKING_DIRECTORY / file_name).open("r") as file:
        lines = file.readlines()
    if start is not None:
        start = 0
    return "\n".join(lines[start:end])


@tool
def write_document(
    content: Annotated[str, "Text content (can be code) to be written into the document."],
    file_name: Annotated[str, "File path to save the document."],
) -> Annotated[str, "Path of the saved document file."]:
    """Create and save a text document (can be code)."""
    with (WORKING_DIRECTORY / file_name).open("w") as file:
        file.write(content)
    return f"Document saved to {file_name}"


@tool
def edit_document(
    file_name: Annotated[str, "Path of the document (can be code) to be edited."],
    inserts: Annotated[
        Dict[int, str],
        "Dictionary where key is the line number (1-indexed) and value is the text to be inserted at that line.",
    ],
) -> Annotated[str, "Path of the edited document file."]:
    """Edit a document by inserting text at specific line numbers."""

    with (WORKING_DIRECTORY / file_name).open("r") as file:
        lines = file.readlines()

    sorted_inserts = sorted(inserts.items())

    for line_number, text in sorted_inserts:
        if 1 <= line_number <= len(lines) + 1:
            lines.insert(line_number - 1, text + "\n")
        else:
            return f"Error: Line number {line_number} is out of range."

    with (WORKING_DIRECTORY / file_name).open("w") as file:
        file.writelines(lines)

    return f"Document edited and saved to {file_name}"


# Warning: This executes code locally, which can be unsafe when not sandboxed

repl = PythonREPL()


@tool
def python_repl(
    code: Annotated[str, "The python code to execute to generate your tools."]
):
    """Use this to execute python code. If you want to see the output of a value,
    you should print it out with `print(...)`. This is visible to the user."""
    try:
        result = repl.run(code)
    except BaseException as e:
        return f"Failed to execute. Error: {repr(e)}"
    return f"Succesfully executed:\n```python\n{code}\n```\nStdout: {result}"


     
/var/folders/d0/f5cdt48n3hbc76z8tk2rpg6w0000gn/T/tmpuy9f9gjf
Helper Utilities
We are going to create a few utility functions to make it more concise when we want to:

Create a worker agent.
Create a supervisor for the sub-graph.
These will simplify the graph compositional code at the end for us so it's easier to see what's going on.


from typing import Any, Callable, List, Optional, TypedDict, Union

from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import Runnable
from langchain_core.tools import BaseTool
from langchain_openai import ChatOpenAI

from langgraph.graph import END, StateGraph


def create_agent(
    llm: ChatOpenAI,
    tools: list,
    system_prompt: str,
) -> str:
    """Create a function-calling agent and add it to the graph."""
    system_prompt += "\nWork autonomously according to your specialty, using the tools available to you."
    " Do not ask for clarification. Take your best guess given the available information."
    " Your other team members (and other teams) will collaborate with you with their own specialties."
    " You are chosen for a reason! You are one of the following team members: {team_members}."
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                system_prompt,
            ),
            MessagesPlaceholder(variable_name="messages"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ]
    )
    agent = create_openai_functions_agent(llm, tools, prompt)
    executor = AgentExecutor(agent=agent, tools=tools)
    return executor


def agent_node(state, agent, name):
    result = agent.invoke(state)
    return {"messages": [HumanMessage(content=result["output"], name=name)]}


def create_team_supervisor(llm: ChatOpenAI, system_prompt, members) -> str:
    """An LLM-based router."""
    options = ["FINISH"] + members
    function_def = {
        "name": "route",
        "description": "Select the next role.",
        "parameters": {
            "title": "routeSchema",
            "type": "object",
            "properties": {
                "next": {
                    "title": "Next",
                    "anyOf": [
                        {"enum": options},
                    ],
                },
            },
            "required": ["next"],
        },
    }
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="messages"),
            (
                "system",
                "Given the conversation above, who should act next?"
                " Or should we FINISH? Select one of: {options}",
            ),
        ]
    ).partial(options=str(options), team_members=", ".join(members))
    return (
        prompt
        | llm.bind_functions(functions=[function_def], function_call="route")
        | JsonOutputFunctionsParser()
    )


     
Define Agent Teams
Now we can get to define our hierachical teams. "Choose your player!"

Research Team
The research team will have a search agent and a web scraping "research_agent" as the two worker nodes. Let's create those, as well as the team supervisor.


import functools
import operator

from langchain_core.messages import AIMessage, BaseMessage, HumanMessage
from langchain_openai.chat_models import ChatOpenAI
import functools


# Research team graph state
class ResearchTeamState(TypedDict):
    # A message is added after each team member finishes
    messages: Annotated[List[BaseMessage], operator.add]
    # The team members are tracked so they are aware of
    # the others' skill-sets
    team_members: List[str]
    # Used to route work. The supervisor calls a function
    # that will update this every time it makes a decision
    next: str


llm = ChatOpenAI(model="gpt-4-1106-preview")

search_agent = create_agent(
    llm,
    [tavily_tool],
    "You are a research assistant who can search for up-to-date info, library documentation, code solutions, or other references using the tavily search engine.",
)
search_node = functools.partial(agent_node, agent=search_agent, name="Search")

research_agent = create_agent(
    llm,
    [scrape_webpages],
    "You are a research assistant who can scrape specified urls for more detailed information using the scrape_webpages function.",
)
research_node = functools.partial(agent_node, agent=research_agent, name="Web Scraper")

supervisor_agent = create_team_supervisor(
    llm,
    "You are a supervisor tasked with managing a conversation between the"
    " following workers:  Search, Web Scraper. Given the following user request,"
    " respond with the worker to act next. Each worker will perform a"
    " task and respond with their results and status. When finished,"
    " respond with FINISH.",
    ["Search", "Web Scraper"],
)


     
Now that we've created the necessary components, defining their interactions is easy. Add the nodes to the team graph, and define the edges, which determine the transition criteria.


research_graph = StateGraph(ResearchTeamState)
research_graph.add_node("Search", search_node)
research_graph.add_node("Web Scraper", research_node)
research_graph.add_node("supervisor", supervisor_agent)

# Define the control flow
research_graph.add_edge("Search", "supervisor")
research_graph.add_edge("Web Scraper", "supervisor")
research_graph.add_conditional_edges(
    "supervisor",
    lambda x: x["next"],
    {"Search": "Search", "Web Scraper": "Web Scraper", "FINISH": END},
)


research_graph.set_entry_point("supervisor")
chain = research_graph.compile()


# The following functions interoperate between the top level graph state
# and the state of the research sub-graph
# this makes it so that the states of each graph don't get intermixed
def enter_chain(message: str):
    results = {
        "messages": [HumanMessage(content=message)],
    }
    return results


research_chain = enter_chain | chain


     
We can give this team work directly. Try it out below.


for s in research_chain.stream(
    """
   Creating a specification for integrating Swaks (Swiss Army Knife for SMTP) with a Large Language Model (LLM) for testing and analyzing SMTP server functionality involves automating the process of sending test emails, capturing the output of these tests, and then processing this data for insights. Swaks is a versatile command-line tool that's ideal for this task, thanks to its ability to simulate various email sending scenarios and capture detailed SMTP session data.

### Specification for Analyzing SMTP Functionality with Swaks and an LLM

#### Objective:
Develop an automated process to use Swaks for generating SMTP test emails, capturing the transaction details, and analyzing these details with an LLM to identify potential issues, optimizations, or confirm functionality of the SMTP setup.

#### Requirements:

1. **Swaks Installation**:
   - Ensure Swaks is installed on the system where the tests will be run. If not installed, Swaks can typically be installed via package managers on Unix-like systems.

2. **SMTP Test Configuration**:
   - Define SMTP server details, including the host, port, and any authentication details if necessary.
   - Specify the sender and recipient email addresses for the test emails.

3. **Automated Swaks Execution**:
   - Use Swaks to send test emails, capturing the detailed output of the SMTP transaction.
   - Optionally, use various Swaks options to simulate different email sending scenarios.

4. **Data Structuring for LLM Analysis**:
   - Process the Swaks output to extract and structure key data points from the SMTP transaction in a format suitable for LLM analysis, such as JSON.

5. **LLM Integration for Analysis**:
   - Send the structured SMTP transaction data to the LLM for analysis.
   - Interpret the LLM's output to extract actionable insights regarding SMTP server performance, configuration issues, or security concerns.

6. **Actionable Insights Implementation**:
   - Based on the LLM's analysis, implement or suggest specific actions to address identified issues or optimizations.

#### Implementation:

```bash
#!/bin/bash

# Define SMTP server details
SMTP_SERVER="smtp.example.com"
SMTP_PORT=25
SENDER="sender@example.com"
RECIPIENT="recipient@example.com"

# Define the path for the structured output file
STRUCTURED_OUTPUT="/path/to/swaks-output.json"

# Use Swaks to send a test email and capture the output
swaks_output=$(swaks --server $SMTP_SERVER --port $SMTP_PORT --to $RECIPIENT --from $SENDER --hide-all)

# Process the Swaks output to extract relevant data and structure it for LLM analysis
# This is a simplified example. You may need to adjust the parsing to fit your needs
echo "{ \"smtp_server\": \"$SMTP_SERVER\", \"smtp_port\": $SMTP_PORT, \"sender\": \"$SENDER\", \"recipient\": \"$RECIPIENT\", \"transaction_details\": \"$swaks_output\" }" > $STRUCTURED_OUTPUT

echo "Structured SMTP transaction data saved to $STRUCTURED_OUTPUT"

# Placeholder for sending structured data to the LLM and retrieving analysis
# You will need to implement this part based on how you interact with the LLM
# Example placeholder:
# python3 analyze_with_llm.py "$STRUCTURED_OUTPUT"

echo "LLM analysis complete. Review recommendations and take action as necessary."
```

Before running this script:
- Ensure Swaks is installed on your system.
- Replace placeholders (`smtp.example.com`, `sender@example.com`, `recipient@example.com`, `/path/to/swaks-output.json`) with actual values relevant to your test.
- The processing of Swaks output in this example is very basic. Depending on your needs and the format of the LLM analysis, you may need to extract and structure the data more precisely.
- Implement the logic for sending structured data to the LLM and processing its output according to the specifics of your LLM's API or interface.

This specification aims to facilitate automated testing and analysis of SMTP functionality using Swaks and an LLM, providing a basis for identifying and addressing issues within your SMTP server setup. """
    , {"recursion_limit": 100}
):
    if "__end__" not in s:
        print(s)
        print("---")


     
{'supervisor': {'next': 'FINISH'}}
---
Document Writing Team
Create the document writing team below using a similar approach. This time, we will give each agent access to different file-writing tools.

Note that we are giving file-system access to our agent here, which is not safe in all cases.


import operator
from pathlib import Path


# Document writing team graph state
class DocWritingState(TypedDict):
    # This tracks the team's conversation internally
    messages: Annotated[List[BaseMessage], operator.add]
    # This provides each worker with context on the others' skill sets
    team_members: str
    # This is how the supervisor tells langgraph who to work next
    next: str
    # This tracks the shared directory state
    current_files: str


# This will be run before each worker agent begins work
# It makes it so they are more aware of the current state
# of the working directory.
def prelude(state):
    written_files = []
    if not WORKING_DIRECTORY.exists():
        WORKING_DIRECTORY.mkdir()
    try:
        written_files = [
            f.relative_to(WORKING_DIRECTORY) for f in WORKING_DIRECTORY.rglob("*")
        ]
    except:
        pass
    if not written_files:
        return {**state, "current_files": "No files written."}
    return {
        **state,
        "current_files": "\nBelow are files your team has written to the directory:\n"
        + "\n".join([f" - {f}" for f in written_files]),
    }


llm = ChatOpenAI(model="gpt-4-1106-preview")

code_writer_agent = create_agent(
    llm,
    [write_document, edit_document, read_document],
    "You are an expert writer of many kinds of code. You write Shell Scripts, Python Scripts, Markdown files, Makefiles, and other types of code as needed.\n"
    # The {current_files} value is populated automatically by the graph state
    "Below are files currently in your directory:\n{current_files}",
)
# Injects current directory working state before each call
context_aware_code_writer_agent = prelude | code_writer_agent
code_writing_node = functools.partial(
    agent_node, agent=context_aware_code_writer_agent, name="Expert Programmer"
)

code_planning_agent = create_agent(
    llm,
    [create_outline, read_document],
    "You are an expert senior programmer tasked with writing a project outline and"
    " taking notes from the documentation you have read, and creating a psudeocode / documentation priming to aid the coder in writing each individual file. Please keep track of any code examples, math equations, tables, formulae, or anything else that you might put on a reference on how to develop the requested tool. {current_files}",
)
context_aware_code_planning_agent = prelude | code_planning_agent
code_planning_node = functools.partial(
    agent_node, agent=context_aware_code_planning_agent, name="Tool Architect"
)

code_critic = create_agent(
    llm,
    [read_document, python_repl],
    "You are an LLM tool critic, tasked with reviewing the code other AIs have written and giving reflective feedback on how to improve the code. Do not add functionality beyond what is specified but do ensure it is not incomplete, needlessly complicated, insecure, or over-broad in scope in such a way as to be potentially dangerous. Try to make sure we don't destroy the world with our code. {current_files}",
)
context_aware_code_critic = prelude | code_critic
chart_generating_node = functools.partial(
    agent_node, agent=context_aware_code_critic, name="Code Critic"
)

code_writing_supervisor = create_team_supervisor(
    llm,
    "You are a supervisor tasked with managing a conversation between the"
    " following teammates:  {team_members}. Given the following user request,"
    " respond with the teammate to act next. Each teammate will perform a"
    " task and respond with their results and status. When finished,"
    " respond with FINISH.",
    ["Expert Programmer", "Tool Architect", "Code Critic"],
)


     

import operator
from pathlib import Path


# Document writing team graph state
class DocWritingState(TypedDict):
    # This tracks the team's conversation internally
    messages: Annotated[List[BaseMessage], operator.add]
    # This provides each worker with context on the others' skill sets
    team_members: str
    # This is how the supervisor tells langgraph who to work next
    next: str
    # This tracks the shared directory state
    current_files: str


# This will be run before each worker agent begins work
# It makes it so they are more aware of the current state
# of the working directory.
def prelude(state):
    written_files = []
    if not WORKING_DIRECTORY.exists():
        WORKING_DIRECTORY.mkdir()
    try:
        written_files = [
            f.relative_to(WORKING_DIRECTORY) for f in WORKING_DIRECTORY.rglob("*")
        ]
    except:
        pass
    if not written_files:
        return {**state, "current_files": "No files written."}
    return {
        **state,
        "current_files": "\nBelow are files your team has written to the directory:\n"
        + "\n".join([f" - {f}" for f in written_files]),
    }


llm = ChatOpenAI(model="gpt-4-1106-preview")

code_writer_agent = create_agent(
    llm,
    [write_document, edit_document, read_document],
    "You are an expert writer of many kinds of code. You write Shell Scripts, Python Scripts, Markdown files, Makefiles, and other types of code as needed.\n"
    # The {current_files} value is populated automatically by the graph state
    "Below are files currently in your directory:\n{current_files}",
)
# Injects current directory working state before each call
context_aware_code_writer_agent = prelude | code_writer_agent
code_writing_node = functools.partial(
    agent_node, agent=context_aware_code_writer_agent, name="Expert Programmer"
)

code_planning_agent = create_agent(
    llm,
    [create_outline, read_document],
    "You are an expert senior programmer tasked with writing a project outline and"
    " taking notes from the documentation you have read, and creating a psudeocode / documentation priming to aid the coder in writing each individual file. Please keep track of any code examples, math equations, tables, formulae, or anything else that you might put on a reference on how to develop the requested tool. {current_files}",
)
context_aware_code_planning_agent = prelude | code_planning_agent
code_planning_node = functools.partial(
    agent_node, agent=context_aware_code_planning_agent, name="Tool Architect"
)

code_critic = create_agent(
    llm,
    [read_document, python_repl],
    "You are an LLM tool critic, tasked with reviewing the code other AIs have written and giving reflective feedback on how to improve the code. Do not add functionality beyond what is specified but do ensure it is not incomplete, needlessly complicated, insecure, or over-broad in scope in such a way as to be potentially dangerous. Try to make sure we don't destroy the world with our code. {current_files}",
)
context_aware_code_critic = prelude | code_critic
chart_generating_node = functools.partial(
    agent_node, agent=context_aware_code_critic, name="Code Critic"
)

code_writing_supervisor = create_team_supervisor(
    llm,
    "You are a supervisor tasked with managing a conversation between the"
    " following teammates:  {team_members}. Given the following user request,"
    " respond with the teammate to act next. Each teammate will perform a"
    " task and respond with their results and status. When finished,"
    " respond with FINISH.",
    ["Expert Programmer", "Tool Architect", "Code Critic"],
)


     

# Create the graph here:
# Note that we have unrolled the loop for the sake of this doc
authoring_graph = StateGraph(DocWritingState)

authoring_graph.add_node("Search", search_node)
authoring_graph.add_node("Web Scraper", research_node)
authoring_graph.add_node("search_supervisor", supervisor_agent)

# Define the control flow
authoring_graph.add_edge("Search", "search_supervisor")
authoring_graph.add_edge("Web Scraper", "search_supervisor")


authoring_graph.add_node("Tool Architect", code_planning_node)
authoring_graph.add_node("Expert Programmer", code_writing_node)
authoring_graph.add_node("Code Critic", chart_generating_node)
authoring_graph.add_node("authoring_supervisor", code_writing_supervisor)

# Add the edges that always occur
authoring_graph.add_edge("Tool Architect", "authoring_supervisor")
authoring_graph.add_edge("Expert Programmer", "authoring_supervisor")
authoring_graph.add_edge("Code Critic", "authoring_supervisor")
authoring_graph.add_edge("authoring_supervisor", "search_supervisor")
authoring_graph.add_edge("search_supervisor", "authoring_supervisor")


## after the search supervisor is done, we can pass on the state to the authoring graph
authoring_graph.add_conditional_edges(
    "search_supervisor",
    lambda x: x["next"],
    {
        "Search": "Search", 
        "Web Scraper": "Web Scraper", 
        "FINISH": "authoring_supervisor"
    },
)

# Add the edges where routing applies
authoring_graph.add_conditional_edges(
    "authoring_supervisor",
    lambda x: x["next"],
    {
        "ENTER": "Tool Architect",
        "Tool Architect": "Expert Programmer",
        "Expert Programmer": "Code Critic",
        "Code Critic": "Expert Programmerrammerrammerrammerrammer",
        "FINISH": END,
    },
)

authoring_graph.set_entry_point("search_supervisor")
chain = research_graph.compile()


# The following functions interoperate between the top level graph state
# and the state of the research sub-graph
# this makes it so that the states of each graph don't get intermixed
def enter_chain(message: str, members: List[str]):
    results = {
        "messages": [HumanMessage(content=message)],
        "team_members": ", ".join(members),
    }
    return results


# We re-use the enter/exit functions to wrap the graph
authoring_chain = (
    functools.partial(enter_chain, members=authoring_graph.nodes)
    | authoring_graph.compile()
)


     
With the objects themselves created, we can form the graph.


for s in authoring_chain.stream(
    """
   Creating a specification for integrating Swaks (Swiss Army Knife for SMTP) with a Large Language Model (LLM) for testing and analyzing SMTP server functionality involves automating the process of sending test emails, capturing the output of these tests, and then processing this data for insights. Swaks is a versatile command-line tool that's ideal for this task, thanks to its ability to simulate various email sending scenarios and capture detailed SMTP session data.

### Specification for Analyzing SMTP Functionality with Swaks and an LLM

#### Objective:
Develop an automated process to use Swaks for generating SMTP test emails, capturing the transaction details, and analyzing these details with an LLM to identify potential issues, optimizations, or confirm functionality of the SMTP setup.

#### Requirements:

1. **Swaks Installation**:
   - Ensure Swaks is installed on the system where the tests will be run. If not installed, Swaks can typically be installed via package managers on Unix-like systems.

2. **SMTP Test Configuration**:
   - Define SMTP server details, including the host, port, and any authentication details if necessary.
   - Specify the sender and recipient email addresses for the test emails.

3. **Automated Swaks Execution**:
   - Use Swaks to send test emails, capturing the detailed output of the SMTP transaction.
   - Optionally, use various Swaks options to simulate different email sending scenarios.

4. **Data Structuring for LLM Analysis**:
   - Process the Swaks output to extract and structure key data points from the SMTP transaction in a format suitable for LLM analysis, such as JSON.

5. **LLM Integration for Analysis**:
   - Send the structured SMTP transaction data to the LLM for analysis.
   - Interpret the LLM's output to extract actionable insights regarding SMTP server performance, configuration issues, or security concerns.

6. **Actionable Insights Implementation**:
   - Based on the LLM's analysis, implement or suggest specific actions to address identified issues or optimizations.

#### Implementation:

```bash
#!/bin/bash

# Define SMTP server details
SMTP_SERVER="smtp.example.com"
SMTP_PORT=25
SENDER="sender@example.com"
RECIPIENT="recipient@example.com"

# Define the path for the structured output file
STRUCTURED_OUTPUT="/path/to/swaks-output.json"

# Use Swaks to send a test email and capture the output
swaks_output=$(swaks --server $SMTP_SERVER --port $SMTP_PORT --to $RECIPIENT --from $SENDER --hide-all)

# Process the Swaks output to extract relevant data and structure it for LLM analysis
# This is a simplified example. You may need to adjust the parsing to fit your needs
echo "{ \"smtp_server\": \"$SMTP_SERVER\", \"smtp_port\": $SMTP_PORT, \"sender\": \"$SENDER\", \"recipient\": \"$RECIPIENT\", \"transaction_details\": \"$swaks_output\" }" > $STRUCTURED_OUTPUT

echo "Structured SMTP transaction data saved to $STRUCTURED_OUTPUT"

# Placeholder for sending structured data to the LLM and retrieving analysis
# You will need to implement this part based on how you interact with the LLM
# Example placeholder:
# python3 analyze_with_llm.py "$STRUCTURED_OUTPUT"

echo "LLM analysis complete. Review recommendations and take action as necessary."
```

Before running this script:
- Ensure Swaks is installed on your system.
- Replace placeholders (`smtp.example.com`, `sender@example.com`, `recipient@example.com`, `/path/to/swaks-output.json`) with actual values relevant to your test.
- The processing of Swaks output in this example is very basic. Depending on your needs and the format of the LLM analysis, you may need to extract and structure the data more precisely.
- Implement the logic for sending structured data to the LLM and processing its output according to the specifics of your LLM's API or interface.

This specification aims to facilitate automated testing and analysis of SMTP functionality using Swaks and an LLM, providing a basis for identifying and addressing issues within your SMTP server setup. 
    """,
    {"recursion_limit": 100},
):
    if "__end__" not in s:
        print(s)
        print("---")


     
{'search_supervisor': {'next': 'FINISH'}}
---
{'authoring_supervisor': {'next': 'Tool Architect'}}
---
Add Layers
In this design, we are enforcing a top-down planning policy. We've created two graphs already, but we have to decide how to route work between the two.

We'll create a third graph to orchestrate the previous two, and add some connectors to define how this top-level state is shared between the different graphs.


# from langchain_core.messages import AIMessage, BaseMessage, HumanMessage
# from langchain_openai.chat_models import ChatOpenAI


# llm = ChatOpenAI(model="gpt-4-1106-preview")

# supervisor_node = create_team_supervisor(
#     llm,
#     "You are a supervisor tasked with managing a conversation between the"
#     " following teams: {team_members}. Given the following user request,"
#     " respond with the worker to act next. Each worker will perform a"
#     " task and respond with their results and status. When finished,"
#     " respond with FINISH.",
#     ["Research team", "Paper writing team"],
# )


     

# Top-level graph state
class State(TypedDict):
    messages: Annotated[List[BaseMessage], operator.add]
    next: str


def get_last_message(state: State) -> str:
    return state["messages"][-1].content


def join_graph(response: dict):
    return {"messages": [response["messages"][-1]]}


# Define the graph.
super_graph = StateGraph(State)
# First add the nodes, which will do the work
super_graph.add_node("Research team", get_last_message | research_chain | join_graph)
super_graph.add_node(
    "Paper writing team", get_last_message | authoring_chain | join_graph
)
super_graph.add_node("supervisor", supervisor_node)

# Define the graph connections, which controls how the logic
# propagates through the program
super_graph.add_edge("Research team", "supervisor")
super_graph.add_edge("Paper writing team", "supervisor")
super_graph.add_conditional_edges(
    "supervisor",
    lambda x: x["next"],
    {
        "Paper writing team": "Paper writing team",
        "Research team": "Research team",
        "FINISH": END,
    },
)
super_graph.set_entry_point("supervisor")
super_graph = super_graph.compile()


     

# for s in super_graph.stream(
#     {
#         "messages": [
#             HumanMessage(
#                 content="Write a brief research report on the North American sturgeon. Include a chart."
#             )
#         ],
#     },
#     {"recursion_limit": 150},
# ):
#     if "__end__" not in s:
#         print(s)
#         print("---")


     
{'supervisor': {'next': 'Research team'}}
---
{'Research team': {'messages': [HumanMessage(content='# Research Report: North American Sturgeon\n\n## Overview\nSturgeons are ancient fish that have existed for approximately 200 million years. They have distinctive features like a heterocercal caudal fin, bony scutes, and a largely cartilaginous skeleton. They are bottom-feeders, with a protrusible mouth that allows them to suck up their food. Sturgeons are known for their roe, which is processed into caviar - a highly prized and expensive product.\n\n## North American Species\nIn North America, notable species of sturgeon include the Pallid Sturgeon (*Scaphirhynchus albus*), White Sturgeon (*Acipenser transmontanus*), Green Sturgeon (*Acipenser medirostris*), and Shortnose Sturgeon (*Acipenser brevirostrum*). The White Sturgeon is the largest freshwater fish in North America, found from Mexico to Alaska, while the Shortnose Sturgeon is found along the eastern coast from New Brunswick to Florida. The Green Sturgeon also inhabits the west coast, from Mexico to Alaska.\n\n## Conservation Status\nMany sturgeon species are threatened or endangered due to overfishing, poaching, habitat destruction, and dam construction. The International Union for Conservation of Nature (IUCN) lists many sturgeon species as at risk, with some, like the Pallid Sturgeon, being critically endangered. \n\n## Regulatory Protections\nAll sturgeons are protected under the Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES). This international agreement restricts the trade of sturgeon and sturgeon products to protect these ancient fish from overexploitation.\n\n## Selected North American Sturgeon Species and Conservation Status\n\n| Species                | Scientific Name             | Distribution (North America)          | Status                            |\n|------------------------|-----------------------------|---------------------------------------|-----------------------------------|\n| White Sturgeon         | *Acipenser transmontanus*   | West coast, from Mexico to Alaska     | Not endangered (some populations) |\n| Shortnose Sturgeon     | *Acipenser brevirostrum*    | East coast, from New Brunswick to FL | Endangered                        |\n| Green Sturgeon         | *Acipenser medirostris*     | West coast, from Mexico to Alaska     | Threatened                        |\n| Pallid Sturgeon        | *Scaphirhynchus albus*      | Central U.S. river systems            | Critically Endangered             |\n\n## Conclusion\nThe North American sturgeons are remarkable survivors from a distant past, but they face significant challenges today. Conservation efforts and international trade regulations are crucial for the survival of these species. Continuous research, habitat protection, and careful management are essential to ensure that these ancient fish continue to thrive in modern times.\n\nPlease note that the status of species can change, and it is always important to consult the most current resources and regulatory documents for up-to-date information on species conservation status and legal protections.\n\n## References\n- [Wikipedia - Sturgeon](https://en.wikipedia.org/wiki/Sturgeon)\n- [Earthwave - Sturgeon](https://www.earthwave.org/sturgeon)\n- [NOAA Fisheries - Green Sturgeon](https://www.fisheries.noaa.gov/species/green-sturgeon)\n- [American Oceans - Types of Sturgeon](https://www.americanoceans.org/facts/types-of-sturgeon/)\n- [Pond Informer - Sturgeon Species](https://pondinformer.com/sturgeon-species/)\n\n(Chart created based on the information gathered from the sources above)', name='Search')]}}
---
{'supervisor': {'next': 'Research team'}}
---
{'Research team': {'messages': [HumanMessage(content="The updated search has provided the following information on the conservation status of North American sturgeons as of 2023:\n\n1. **NOAA Fisheries Updates**: NOAA Fisheries has been actively updating documents and conducting reviews related to various sturgeon species, including the Atlantic and Green sturgeons. For example, there are several 5-Year Reviews listed for the Atlantic Sturgeon's Distinct Population Segments (DPS), which analyze the species' status to ensure that the listing classifications under the Endangered Species Act (ESA) are accurate. These reviews are crucial for informed management and conservation efforts ([NOAA Fisheries Atlantic Sturgeon](https://www.fisheries.noaa.gov/species/atlantic-sturgeon)).\n\n2. **Research on Atlantic Sturgeon**: There has been ongoing research on the Atlantic Sturgeon, with grants being awarded to study their populations and threats, such as vessel strikes. Understanding the genetic diversity and the impact of human activities on these species is vital for their conservation ([NY Times Article](https://www.nytimes.com/2023/02/02/magazine/they-outlasted-the-dinosaurs-can-they-survive-us.html)).\n\n3. **Green Sturgeon Management**: NOAA Fisheries has updated information on the Green Sturgeon as recently as April 10, 2023. They've released a Final Recovery Plan for the Southern DPS of the Green Sturgeon, which outlines strategies to recover this threatened species. Critical habitats have been designated for the conservation of Green Sturgeon populations, indicating a focused effort to protect their ecosystems ([NOAA Fisheries Green Sturgeon](https://www.fisheries.noaa.gov/species/green-sturgeon/conservation-management)).\n\n4. **North American Sturgeon and Paddlefish Society**: The NASPS is calling for a meeting location in 2024 and continues to work towards the conservation and restoration of sturgeon species in North America. This society plays a key role in advancing research and management practices for sturgeon conservation ([NASPS](https://nasps-sturgeon.org/)).\n\n5. **Lake Sturgeon Conservation**: An article from Great Lakes Now reports that Lake Sturgeon has been added to the endangered list, yet there is optimism as conservation efforts are underway. Educational programs and species management are part of the initiatives to protect these ancient fish, highlighting the importance of community involvement in conservation ([Great Lakes Now](https://www.greatlakesnow.org/2023/02/lake-sturgeon-added-to-endangered-list-but-things-are-looking-up/)).\n\nThe information from these sources confirms that conservation status and efforts for North American sturgeons remain a high priority, with ongoing research, management plans, and recovery efforts being implemented to ensure their survival.", name='Search')]}}
---
{'supervisor': {'next': 'Paper writing team'}}
---
{'Paper writing team': {'messages': [HumanMessage(content="The updated search has provided the following information on the conservation status of North American sturgeons as of 2023:\n\n1. **NOAA Fisheries Updates**: NOAA Fisheries has been actively updating documents and conducting reviews related to various sturgeon species, including the Atlantic and Green sturgeons. For example, there are several 5-Year Reviews listed for the Atlantic Sturgeon's Distinct Population Segments (DPS), which analyze the species' status to ensure that the listing classifications under the Endangered Species Act (ESA) are accurate. These reviews are crucial for informed management and conservation efforts ([NOAA Fisheries Atlantic Sturgeon](https://www.fisheries.noaa.gov/species/atlantic-sturgeon)).\n\n2. **Research on Atlantic Sturgeon**: There has been ongoing research on the Atlantic Sturgeon, with grants being awarded to study their populations and threats, such as vessel strikes. Understanding the genetic diversity and the impact of human activities on these species is vital for their conservation ([NY Times Article](https://www.nytimes.com/2023/02/02/magazine/they-outlasted-the-dinosaurs-can-they-survive-us.html)).\n\n3. **Green Sturgeon Management**: NOAA Fisheries has updated information on the Green Sturgeon as recently as April 10, 2023. They've released a Final Recovery Plan for the Southern DPS of the Green Sturgeon, which outlines strategies to recover this threatened species. Critical habitats have been designated for the conservation of Green Sturgeon populations, indicating a focused effort to protect their ecosystems ([NOAA Fisheries Green Sturgeon](https://www.fisheries.noaa.gov/species/green-sturgeon/conservation-management)).\n\n4. **North American Sturgeon and Paddlefish Society**: The NASPS is calling for a meeting location in 2024 and continues to work towards the conservation and restoration of sturgeon species in North America. This society plays a key role in advancing research and management practices for sturgeon conservation ([NASPS](https://nasps-sturgeon.org/)).\n\n5. **Lake Sturgeon Conservation**: An article from Great Lakes Now reports that Lake Sturgeon has been added to the endangered list, yet there is optimism as conservation efforts are underway. Educational programs and species management are part of the initiatives to protect these ancient fish, highlighting the importance of community involvement in conservation ([Great Lakes Now](https://www.greatlakesnow.org/2023/02/lake-sturgeon-added-to-endangered-list-but-things-are-looking-up/)).\n\nThe information from these sources confirms that conservation status and efforts for North American sturgeons remain a high priority, with ongoing research, management plans, and recovery efforts being implemented to ensure their survival.")]}}
---
{'supervisor': {'next': 'Research team'}}
---
{'Research team': {'messages': [HumanMessage(content="The updated search has provided the following information on the conservation status of North American sturgeons as of 2023:\n\n1. **NOAA Fisheries Updates**: NOAA Fisheries has been actively updating documents and conducting reviews related to various sturgeon species, including the Atlantic and Green sturgeons. For example, there are several 5-Year Reviews listed for the Atlantic Sturgeon's Distinct Population Segments (DPS), which analyze the species' status to ensure that the listing classifications under the Endangered Species Act (ESA) are accurate. These reviews are crucial for informed management and conservation efforts ([NOAA Fisheries Atlantic Sturgeon](https://www.fisheries.noaa.gov/species/atlantic-sturgeon)).\n\n2. **Research on Atlantic Sturgeon**: There has been ongoing research on the Atlantic Sturgeon, with grants being awarded to study their populations and threats, such as vessel strikes. Understanding the genetic diversity and the impact of human activities on these species is vital for their conservation ([NY Times Article](https://www.nytimes.com/2023/02/02/magazine/they-outlasted-the-dinosaurs-can-they-survive-us.html)).\n\n3. **Green Sturgeon Management**: NOAA Fisheries has updated information on the Green Sturgeon as recently as April 10, 2023. They've released a Final Recovery Plan for the Southern DPS of the Green Sturgeon, which outlines strategies to recover this threatened species. Critical habitats have been designated for the conservation of Green Sturgeon populations, indicating a focused effort to protect their ecosystems ([NOAA Fisheries Green Sturgeon](https://www.fisheries.noaa.gov/species/green-sturgeon/conservation-management)).\n\n4. **North American Sturgeon and Paddlefish Society**: The NASPS is calling for a meeting location in 2024 and continues to work towards the conservation and restoration of sturgeon species in North America. This society plays a key role in advancing research and management practices for sturgeon conservation ([NASPS](https://nasps-sturgeon.org/)).\n\n5. **Lake Sturgeon Conservation**: An article from Great Lakes Now reports that Lake Sturgeon has been added to the endangered list, yet there is optimism as conservation efforts are underway. Educational programs and species management are part of the initiatives to protect these ancient fish, highlighting the importance of community involvement in conservation ([Great Lakes Now](https://www.greatlakesnow.org/2023/02/lake-sturgeon-added-to-endangered-list-but-things-are-looking-up/)).\n\nThe information from these sources confirms that conservation status and efforts for North American sturgeons remain a high priority, with ongoing research, management plans, and recovery efforts being implemented to ensure their survival.")]}}
---
{'supervisor': {'next': 'Research team'}}
---
{'Research team': {'messages': [HumanMessage(content="Based on the search results, here are some key points regarding the conservation status of North American sturgeons as of 2023:\n\n1. **Legislation**: A bill was introduced in the U.S. Senate by Sen. Rick Scott [R-FL] on September 20, 2023, which is related to environmental issues and is currently with the Senate Environment and Public Works Committee. The specifics of the bill are not provided in the search results, but it could potentially have implications for sturgeon conservation (Source: [Congress.gov](https://www.congress.gov/bill/118th-congress/senate-bill/2870?s=1&r=70)).\n\n2. **Research Grants**: A grant was awarded to Dr. Dewayne Fox and his colleagues for a novel study of Atlantic sturgeon. This research is aimed at understanding the impact of vessel strikes on sturgeon populations, which are a significant threat to these fish. The grant was funded by the National Marine Fisheries Service (NMFS) (Source: [New York Times](https://www.nytimes.com/2023/02/02/magazine/they-outlasted-the-dinosaurs-can-they-survive-us.html)).\n\n3. **North American Sturgeon and Paddlefish Society (NASPS)**: NASPS is actively involved in promoting the conservation and restoration of sturgeon species in North America. They are planning for a meeting location in 2024, indicating ongoing efforts to advance research and management practices for sturgeon conservation (Source: [NASPS](https://nasps-sturgeon.org/)).\n\n4. **Atlantic Sturgeon Habitat**: The Atlantic sturgeon is found along the east coast of North America, from southern Canada to northern Florida. Conservation efforts for this species would involve protecting their habitats in coastal estuaries and rivers where they spawn (Source: [U.S. Fish and Wildlife Service](https://www.fws.gov/sites/default/files/documents/atlantic-sturgeon.pdf)).\n\n5. **Conservation Perspectives**: An article in the North American Journal of Fisheries Management discusses the decades of global sturgeon conservation efforts and the threats posed by an expanding captive culture industry, which could affect wild populations. This perspective highlights the need for careful management of both wild and cultured sturgeon populations to ensure the species' long-term survival (Source: [Wiley Online Library](https://afspubs.onlinelibrary.wiley.com/doi/10.1002/fsh.10865)).\n\nThese search results underscore the continued emphasis on research, legislation, and society-driven efforts to conserve North American sturgeon species. They also highlight some of the challenges that conservationists face, such as the impact of human activities on sturgeon populations and the potential risks associated with aquaculture.", name='Search')]}}
---
{'supervisor': {'next': 'Research team'}}
---
{'Research team': {'messages': [HumanMessage(content="Based on the search results, here are some key points regarding the conservation status of North American sturgeons as of 2023:\n\n1. **Legislation**: A bill was introduced in the U.S. Senate by Sen. Rick Scott [R-FL] on September 20, 2023, which is related to environmental issues and is currently with the Senate Environment and Public Works Committee. The specifics of the bill are not provided in the search results, but it could potentially have implications for sturgeon conservation (Source: [Congress.gov](https://www.congress.gov/bill/118th-congress/senate-bill/2870?s=1&r=70)).\n\n2. **Research Grants**: A grant was awarded to Dr. Dewayne Fox and his colleagues for a novel study of Atlantic sturgeon. This research is aimed at understanding the impact of vessel strikes on sturgeon populations, which are a significant threat to these fish. The grant was funded by the National Marine Fisheries Service (NMFS) (Source: [New York Times](https://www.nytimes.com/2023/02/02/magazine/they-outlasted-the-dinosaurs-can-they-survive-us.html)).\n\n3. **North American Sturgeon and Paddlefish Society (NASPS)**: NASPS is actively involved in promoting the conservation and restoration of sturgeon species in North America. They are planning for a meeting location in 2024, indicating ongoing efforts to advance research and management practices for sturgeon conservation (Source: [NASPS](https://nasps-sturgeon.org/)).\n\n4. **Atlantic Sturgeon Habitat**: The Atlantic sturgeon is found along the east coast of North America, from southern Canada to northern Florida. Conservation efforts for this species would involve protecting their habitats in coastal estuaries and rivers where they spawn (Source: [U.S. Fish and Wildlife Service](https://www.fws.gov/sites/default/files/documents/atlantic-sturgeon.pdf)).\n\n5. **Conservation Perspectives**: An article in the North American Journal of Fisheries Management discusses the decades of global sturgeon conservation efforts and the threats posed by an expanding captive culture industry, which could affect wild populations. This perspective highlights the need for careful management of both wild and cultured sturgeon populations to ensure the species' long-term survival (Source: [Wiley Online Library](https://afspubs.onlinelibrary.wiley.com/doi/10.1002/fsh.10865)).\n\nThese search results underscore the continued emphasis on research, legislation, and society-driven efforts to conserve North American sturgeon species. They also highlight some of the challenges that conservationists face, such as the impact of human activities on sturgeon populations and the potential risks associated with aquaculture.")]}}
---
{'supervisor': {'next': 'Research team'}}
---
{'Research team': {'messages': [HumanMessage(content="Based on the search results, here are some key points regarding the conservation status of North American sturgeons as of 2023:\n\n1. **Legislation**: A bill was introduced in the U.S. Senate by Sen. Rick Scott [R-FL] on September 20, 2023, which is related to environmental issues and is currently with the Senate Environment and Public Works Committee. The specifics of the bill are not provided in the search results, but it could potentially have implications for sturgeon conservation (Source: [Congress.gov](https://www.congress.gov/bill/118th-congress/senate-bill/2870?s=1&r=70)).\n\n2. **Research Grants**: A grant was awarded to Dr. Dewayne Fox and his colleagues for a novel study of Atlantic sturgeon. This research is aimed at understanding the impact of vessel strikes on sturgeon populations, which are a significant threat to these fish. The grant was funded by the National Marine Fisheries Service (NMFS) (Source: [New York Times](https://www.nytimes.com/2023/02/02/magazine/they-outlasted-the-dinosaurs-can-they-survive-us.html)).\n\n3. **North American Sturgeon and Paddlefish Society (NASPS)**: NASPS is actively involved in promoting the conservation and restoration of sturgeon species in North America. They are planning for a meeting location in 2024, indicating ongoing efforts to advance research and management practices for sturgeon conservation (Source: [NASPS](https://nasps-sturgeon.org/)).\n\n4. **Atlantic Sturgeon Habitat**: The Atlantic sturgeon is found along the east coast of North America, from southern Canada to northern Florida. Conservation efforts for this species would involve protecting their habitats in coastal estuaries and rivers where they spawn (Source: [U.S. Fish and Wildlife Service](https://www.fws.gov/sites/default/files/documents/atlantic-sturgeon.pdf)).\n\n5. **Conservation Perspectives**: An article in the North American Journal of Fisheries Management discusses the decades of global sturgeon conservation efforts and the threats posed by an expanding captive culture industry, which could affect wild populations. This perspective highlights the need for careful management of both wild and cultured sturgeon populations to ensure the species' long-term survival (Source: [Wiley Online Library](https://afspubs.onlinelibrary.wiley.com/doi/10.1002/fsh.10865)).\n\nThese search results underscore the continued emphasis on research, legislation, and society-driven efforts to conserve North American sturgeon species. They also highlight some of the challenges that conservationists face, such as the impact of human activities on sturgeon populations and the potential risks associated with aquaculture.")]}}
---
{'supervisor': {'next': 'Research team'}}
---
{'Research team': {'messages': [HumanMessage(content="Based on the search results, here are some key points regarding the conservation status of North American sturgeons as of 2023:\n\n1. **Legislation**: A bill was introduced in the U.S. Senate by Sen. Rick Scott [R-FL] on September 20, 2023, which is related to environmental issues and is currently with the Senate Environment and Public Works Committee. The specifics of the bill are not provided in the search results, but it could potentially have implications for sturgeon conservation (Source: [Congress.gov](https://www.congress.gov/bill/118th-congress/senate-bill/2870?s=1&r=70)).\n\n2. **Research Grants**: A grant was awarded to Dr. Dewayne Fox and his colleagues for a novel study of Atlantic sturgeon. This research is aimed at understanding the impact of vessel strikes on sturgeon populations, which are a significant threat to these fish. The grant was funded by the National Marine Fisheries Service (NMFS) (Source: [New York Times](https://www.nytimes.com/2023/02/02/magazine/they-outlasted-the-dinosaurs-can-they-survive-us.html)).\n\n3. **North American Sturgeon and Paddlefish Society (NASPS)**: NASPS is actively involved in promoting the conservation and restoration of sturgeon species in North America. They are planning for a meeting location in 2024, indicating ongoing efforts to advance research and management practices for sturgeon conservation (Source: [NASPS](https://nasps-sturgeon.org/)).\n\n4. **Atlantic Sturgeon Habitat**: The Atlantic sturgeon is found along the east coast of North America, from southern Canada to northern Florida. Conservation efforts for this species would involve protecting their habitats in coastal estuaries and rivers where they spawn (Source: [U.S. Fish and Wildlife Service](https://www.fws.gov/sites/default/files/documents/atlantic-sturgeon.pdf)).\n\n5. **Conservation Perspectives**: An article in the North American Journal of Fisheries Management discusses the decades of global sturgeon conservation efforts and the threats posed by an expanding captive culture industry, which could affect wild populations. This perspective highlights the need for careful management of both wild and cultured sturgeon populations to ensure the species' long-term survival (Source: [Wiley Online Library](https://afspubs.onlinelibrary.wiley.com/doi/10.1002/fsh.10865)).\n\nThese search results underscore the continued emphasis on research, legislation, and society-driven efforts to conserve North American sturgeon species. They also highlight some of the challenges that conservationists face, such as the impact of human activities on sturgeon populations and the potential risks associated with aquaculture.")]}}
---
{'supervisor': {'next': 'Research team'}}
---
{'Research team': {'messages': [HumanMessage(content="Based on the search results, here are some key points regarding the conservation status of North American sturgeons as of 2023:\n\n1. **Legislation**: A bill was introduced in the U.S. Senate by Sen. Rick Scott [R-FL] on September 20, 2023, which is related to environmental issues and is currently with the Senate Environment and Public Works Committee. The specifics of the bill are not provided in the search results, but it could potentially have implications for sturgeon conservation (Source: [Congress.gov](https://www.congress.gov/bill/118th-congress/senate-bill/2870?s=1&r=70)).\n\n2. **Research Grants**: A grant was awarded to Dr. Dewayne Fox and his colleagues for a novel study of Atlantic sturgeon. This research is aimed at understanding the impact of vessel strikes on sturgeon populations, which are a significant threat to these fish. The grant was funded by the National Marine Fisheries Service (NMFS) (Source: [New York Times](https://www.nytimes.com/2023/02/02/magazine/they-outlasted-the-dinosaurs-can-they-survive-us.html)).\n\n3. **North American Sturgeon and Paddlefish Society (NASPS)**: NASPS is actively involved in promoting the conservation and restoration of sturgeon species in North America. They are planning for a meeting location in 2024, indicating ongoing efforts to advance research and management practices for sturgeon conservation (Source: [NASPS](https://nasps-sturgeon.org/)).\n\n4. **Atlantic Sturgeon Habitat**: The Atlantic sturgeon is found along the east coast of North America, from southern Canada to northern Florida. Conservation efforts for this species would involve protecting their habitats in coastal estuaries and rivers where they spawn (Source: [U.S. Fish and Wildlife Service](https://www.fws.gov/sites/default/files/documents/atlantic-sturgeon.pdf)).\n\n5. **Conservation Perspectives**: An article in the North American Journal of Fisheries Management discusses the decades of global sturgeon conservation efforts and the threats posed by an expanding captive culture industry, which could affect wild populations. This perspective highlights the need for careful management of both wild and cultured sturgeon populations to ensure the species' long-term survival (Source: [Wiley Online Library](https://afspubs.onlinelibrary.wiley.com/doi/10.1002/fsh.10865)).\n\nThese search results underscore the continued emphasis on research, legislation, and society-driven efforts to conserve North American sturgeon species. They also highlight some of the challenges that conservationists face, such as the impact of human activities on sturgeon populations and the potential risks associated with aquaculture.")]}}
---
{'supervisor': {'next': 'Research team'}}
---
{'Research team': {'messages': [HumanMessage(content="Based on the search results, here are some key points regarding the conservation status of North American sturgeons as of 2023:\n\n1. **Legislation**: A bill was introduced in the U.S. Senate by Sen. Rick Scott [R-FL] on September 20, 2023, which is related to environmental issues and is currently with the Senate Environment and Public Works Committee. The specifics of the bill are not provided in the search results, but it could potentially have implications for sturgeon conservation (Source: [Congress.gov](https://www.congress.gov/bill/118th-congress/senate-bill/2870?s=1&r=70)).\n\n2. **Research Grants**: A grant was awarded to Dr. Dewayne Fox and his colleagues for a novel study of Atlantic sturgeon. This research is aimed at understanding the impact of vessel strikes on sturgeon populations, which are a significant threat to these fish. The grant was funded by the National Marine Fisheries Service (NMFS) (Source: [New York Times](https://www.nytimes.com/2023/02/02/magazine/they-outlasted-the-dinosaurs-can-they-survive-us.html)).\n\n3. **North American Sturgeon and Paddlefish Society (NASPS)**: NASPS is actively involved in promoting the conservation and restoration of sturgeon species in North America. They are planning for a meeting location in 2024, indicating ongoing efforts to advance research and management practices for sturgeon conservation (Source: [NASPS](https://nasps-sturgeon.org/)).\n\n4. **Atlantic Sturgeon Habitat**: The Atlantic sturgeon is found along the east coast of North America, from southern Canada to northern Florida. Conservation efforts for this species would involve protecting their habitats in coastal estuaries and rivers where they spawn (Source: [U.S. Fish and Wildlife Service](https://www.fws.gov/sites/default/files/documents/atlantic-sturgeon.pdf)).\n\n5. **Conservation Perspectives**: An article in the North American Journal of Fisheries Management discusses the decades of global sturgeon conservation efforts and the threats posed by an expanding captive culture industry, which could affect wild populations. This perspective highlights the need for careful management of both wild and cultured sturgeon populations to ensure the species' long-term survival (Source: [Wiley Online Library](https://afspubs.onlinelibrary.wiley.com/doi/10.1002/fsh.10865)).\n\nThese search results underscore the continued emphasis on research, legislation, and society-driven efforts to conserve North American sturgeon species. They also highlight some of the challenges that conservationists face, such as the impact of human activities on sturgeon populations and the potential risks associated with aquaculture.")]}}
---
{'supervisor': {'next': 'Research team'}}
---
{'Research team': {'messages': [HumanMessage(content="Based on the search results, here are some key points regarding the conservation status of North American sturgeons as of 2023:\n\n1. **Legislation**: A bill was introduced in the U.S. Senate by Sen. Rick Scott [R-FL] on September 20, 2023, which is related to environmental issues and is currently with the Senate Environment and Public Works Committee. The specifics of the bill are not provided in the search results, but it could potentially have implications for sturgeon conservation (Source: [Congress.gov](https://www.congress.gov/bill/118th-congress/senate-bill/2870?s=1&r=70)).\n\n2. **Research Grants**: A grant was awarded to Dr. Dewayne Fox and his colleagues for a novel study of Atlantic sturgeon. This research is aimed at understanding the impact of vessel strikes on sturgeon populations, which are a significant threat to these fish. The grant was funded by the National Marine Fisheries Service (NMFS) (Source: [New York Times](https://www.nytimes.com/2023/02/02/magazine/they-outlasted-the-dinosaurs-can-they-survive-us.html)).\n\n3. **North American Sturgeon and Paddlefish Society (NASPS)**: NASPS is actively involved in promoting the conservation and restoration of sturgeon species in North America. They are planning for a meeting location in 2024, indicating ongoing efforts to advance research and management practices for sturgeon conservation (Source: [NASPS](https://nasps-sturgeon.org/)).\n\n4. **Atlantic Sturgeon Habitat**: The Atlantic sturgeon is found along the east coast of North America, from southern Canada to northern Florida. Conservation efforts for this species would involve protecting their habitats in coastal estuaries and rivers where they spawn (Source: [U.S. Fish and Wildlife Service](https://www.fws.gov/sites/default/files/documents/atlantic-sturgeon.pdf)).\n\n5. **Conservation Perspectives**: An article in the North American Journal of Fisheries Management discusses the decades of global sturgeon conservation efforts and the threats posed by an expanding captive culture industry, which could affect wild populations. This perspective highlights the need for careful management of both wild and cultured sturgeon populations to ensure the species' long-term survival (Source: [Wiley Online Library](https://afspubs.onlinelibrary.wiley.com/doi/10.1002/fsh.10865)).\n\nThese search results underscore the continued emphasis on research, legislation, and society-driven efforts to conserve North American sturgeon species. They also highlight some of the challenges that conservationists face, such as the impact of human activities on sturgeon populations and the potential risks associated with aquaculture.")]}}
---
{'supervisor': {'next': 'Research team'}}
---
{'Research team': {'messages': [HumanMessage(content='The conservation status of North American sturgeons in 2023 is highlighted by several important points:\n\n1. **Research Funding and Studies**: A significant grant was provided to Dr. Dewayne Fox and his colleagues to study the Atlantic sturgeon, with a focus on the impact of vessel strikes which are a major threat to this species. The grant, amounting to $214,730, was funded by the National Marine Fisheries Service (NMFS) ([New York Times](https://www.nytimes.com/2023/02/02/magazine/they-outlasted-the-dinosaurs-can-they-survive-us.html)).\n\n2. **Habitat and Distribution**: The Atlantic sturgeon, an anadromous fish species, has a broad distribution from Labrador, Canada, to Florida, United States. Historically, they may have spawned in at least 35 rivers along the east coast of North America, but today, only 25 rivers are suspected to still support naturally reproducing populations ([USGS](https://pubs.usgs.gov/publication/ofr20231054/full)).\n\n3. **Society Involvement**: The North American Sturgeon and Paddlefish Society (NASPS) is committed to the conservation and restoration of sturgeon species in North America. The society is actively working on developing research related to the biology, management, and utilization of these species. They are also calling for meeting locations for 2024, indicating ongoing efforts to advance sturgeon conservation ([NASPS](https://nasps-sturgeon.org/)).\n\n4. **Conservation Challenges**: An article in the North American Journal of Fisheries Management expresses concerns about the threats to global sturgeon conservation efforts posed by an expanding captive culture industry. This raises the need for careful management of both wild and cultured sturgeon populations ([Wiley Online Library](https://afspubs.onlinelibrary.wiley.com/doi/10.1002/fsh.10865)).\n\n5. **Endangered Species Listings**: Lake sturgeon, one of the largest and oldest species of fish in the Great Lakes, has been added to the endangered list. Despite this, there are indications that the situation for this species might be improving, suggesting successful conservation actions ([Great Lakes Now](https://www.greatlakesnow.org/2023/02/lake-sturgeon-added-to-endangered-list-but-things-are-looking-up/)).\n\nThese points underscore the ongoing efforts to conserve North American sturgeons, the challenges faced by these ancient species, and the critical importance of continued research, funding, and collaborative society work towards their conservation.', name='Search')]}}
---
{'supervisor': {'next': 'Paper writing team'}}
---
{'Paper writing team': {'messages': [HumanMessage(content="Thank you for providing a detailed overview of the conservation status of North American sturgeons in 2023. It's clear that a combination of research funding, understanding of habitat and distribution, societal involvement, awareness of conservation challenges, and endangered species listings are all critical components in the efforts to protect these ancient fish species. \n\nIf you need more detailed information on any of these points or additional research on the current state of North American sturgeons, please let me know, and I can use the tavily search engine to find the most up-to-date data and research available.", name='Search')]}}
---
{'supervisor': {'next': 'Research team'}}
---
{'Research team': {'messages': [HumanMessage(content="Thank you for providing a detailed overview of the conservation status of North American sturgeons in 2023. It's clear that a combination of research funding, understanding of habitat and distribution, societal involvement, awareness of conservation challenges, and endangered species listings are all critical components in the efforts to protect these ancient fish species. \n\nIf you need more detailed information on any of these points or additional research on the current state of North American sturgeons, please let me know, and I can use the tavily search engine to find the most up-to-date data and research available.")]}}
---
{'supervisor': {'next': 'Research team'}}
---
{'Research team': {'messages': [HumanMessage(content="Thank you for providing a detailed overview of the conservation status of North American sturgeons in 2023. It's clear that a combination of research funding, understanding of habitat and distribution, societal involvement, awareness of conservation challenges, and endangered species listings are all critical components in the efforts to protect these ancient fish species. \n\nIf you need more detailed information on any of these points or additional research on the current state of North American sturgeons, please let me know, and I can use the tavily search engine to find the most up-to-date data and research available.")]}}
---
{'supervisor': {'next': 'Research team'}}
---
{'Research team': {'messages': [HumanMessage(content="The conservation status of North American sturgeons in 2023 indicates ongoing challenges and efforts in their preservation. Here's a summary based on recent sources:\n\n1. **Expanding Captive Culture Industry Threat**: An article from the North American Journal of Fisheries Management published in February 2023 discusses how decades of global sturgeon conservation efforts are threatened by an expanding captive culture industry.\n\n2. **Lake Sturgeon Endangered**: Great Lakes Now reported in February 2023 that lake sturgeon have been added to the endangered list. However, there's optimism due to various conservation programs, including educational programs by the Little Traverse Bay Band of Odawa Indians, which involve caring for and releasing sturgeon into the wild.\n\n3. **North American Sturgeon and Paddlefish Society (NASPS)**: NASPS aims to promote the conservation and restoration of sturgeon species in North America by advancing research on their biology, management, and utilization. They hold meetings and symposiums, the latest being a symposium at the American Fisheries Society (AFS) 2023 meeting.\n\n4. **Research and Population Assessments**: The U.S. Geological Survey has published papers reviewing the biology, fisheries, and conservation status of Atlantic sturgeon, providing insights into their population and distribution along the U.S. Atlantic Coast.\n\n5. **Regulatory and Conservation Efforts**: There are ongoing efforts by organizations such as the Wildlife Conservation Society of Canada and the U.S. Fish and Wildlife Service to protect sturgeon species. These efforts include endangered species listings, field research, and public engagement in conservation activities.\n\nThese points highlight the importance of continued research funding, understanding of habitat and distribution, societal involvement, awareness of conservation challenges, and endangered species listings in the efforts to protect North American sturgeons.", name='Search')]}}
---
{'supervisor': {'next': 'Paper writing team'}}
---
{'Paper writing team': {'messages': [HumanMessage(content="The conservation status of North American sturgeons in 2023 indicates ongoing challenges and efforts in their preservation. Here's a summary based on recent sources:\n\n1. **Expanding Captive Culture Industry Threat**: An article from the North American Journal of Fisheries Management published in February 2023 discusses how decades of global sturgeon conservation efforts are threatened by an expanding captive culture industry.\n\n2. **Lake Sturgeon Endangered**: Great Lakes Now reported in February 2023 that lake sturgeon have been added to the endangered list. However, there's optimism due to various conservation programs, including educational programs by the Little Traverse Bay Band of Odawa Indians, which involve caring for and releasing sturgeon into the wild.\n\n3. **North American Sturgeon and Paddlefish Society (NASPS)**: NASPS aims to promote the conservation and restoration of sturgeon species in North America by advancing research on their biology, management, and utilization. They hold meetings and symposiums, the latest being a symposium at the American Fisheries Society (AFS) 2023 meeting.\n\n4. **Research and Population Assessments**: The U.S. Geological Survey has published papers reviewing the biology, fisheries, and conservation status of Atlantic sturgeon, providing insights into their population and distribution along the U.S. Atlantic Coast.\n\n5. **Regulatory and Conservation Efforts**: There are ongoing efforts by organizations such as the Wildlife Conservation Society of Canada and the U.S. Fish and Wildlife Service to protect sturgeon species. These efforts include endangered species listings, field research, and public engagement in conservation activities.\n\nThese points highlight the importance of continued research funding, understanding of habitat and distribution, societal involvement, awareness of conservation challenges, and endangered species listings in the efforts to protect North American sturgeons.")]}}
---
{'supervisor': {'next': 'Paper writing team'}}
---
{'Paper writing team': {'messages': [HumanMessage(content="The conservation status of North American sturgeons in 2023 indicates ongoing challenges and efforts in their preservation. Here's a summary based on recent sources:\n\n1. **Expanding Captive Culture Industry Threat**: An article from the North American Journal of Fisheries Management published in February 2023 discusses how decades of global sturgeon conservation efforts are threatened by an expanding captive culture industry.\n\n2. **Lake Sturgeon Endangered**: Great Lakes Now reported in February 2023 that lake sturgeon have been added to the endangered list. However, there's optimism due to various conservation programs, including educational programs by the Little Traverse Bay Band of Odawa Indians, which involve caring for and releasing sturgeon into the wild.\n\n3. **North American Sturgeon and Paddlefish Society (NASPS)**: NASPS aims to promote the conservation and restoration of sturgeon species in North America by advancing research on their biology, management, and utilization. They hold meetings and symposiums, the latest being a symposium at the American Fisheries Society (AFS) 2023 meeting.\n\n4. **Research and Population Assessments**: The U.S. Geological Survey has published papers reviewing the biology, fisheries, and conservation status of Atlantic sturgeon, providing insights into their population and distribution along the U.S. Atlantic Coast.\n\n5. **Regulatory and Conservation Efforts**: There are ongoing efforts by organizations such as the Wildlife Conservation Society of Canada and the U.S. Fish and Wildlife Service to protect sturgeon species. These efforts include endangered species listings, field research, and public engagement in conservation activities.\n\nThese points highlight the importance of continued research funding, understanding of habitat and distribution, societal involvement, awareness of conservation challenges, and endangered species listings in the efforts to protect North American sturgeons.")]}}
---
{'supervisor': {'next': 'Paper writing team'}}
---
{'Paper writing team': {'messages': [HumanMessage(content="The conservation status of North American sturgeons in 2023 indicates ongoing challenges and efforts in their preservation. Here's a summary based on recent sources:\n\n1. **Expanding Captive Culture Industry Threat**: An article from the North American Journal of Fisheries Management published in February 2023 discusses how decades of global sturgeon conservation efforts are threatened by an expanding captive culture industry.\n\n2. **Lake Sturgeon Endangered**: Great Lakes Now reported in February 2023 that lake sturgeon have been added to the endangered list. However, there's optimism due to various conservation programs, including educational programs by the Little Traverse Bay Band of Odawa Indians, which involve caring for and releasing sturgeon into the wild.\n\n3. **North American Sturgeon and Paddlefish Society (NASPS)**: NASPS aims to promote the conservation and restoration of sturgeon species in North America by advancing research on their biology, management, and utilization. They hold meetings and symposiums, the latest being a symposium at the American Fisheries Society (AFS) 2023 meeting.\n\n4. **Research and Population Assessments**: The U.S. Geological Survey has published papers reviewing the biology, fisheries, and conservation status of Atlantic sturgeon, providing insights into their population and distribution along the U.S. Atlantic Coast.\n\n5. **Regulatory and Conservation Efforts**: There are ongoing efforts by organizations such as the Wildlife Conservation Society of Canada and the U.S. Fish and Wildlife Service to protect sturgeon species. These efforts include endangered species listings, field research, and public engagement in conservation activities.\n\nThese points highlight the importance of continued research funding, understanding of habitat and distribution, societal involvement, awareness of conservation challenges, and endangered species listings in the efforts to protect North American sturgeons.")]}}
---
{'supervisor': {'next': 'Paper writing team'}}
---
{'Paper writing team': {'messages': [HumanMessage(content="The conservation status of North American sturgeons in 2023 indicates ongoing challenges and efforts in their preservation. Here's a summary based on recent sources:\n\n1. **Expanding Captive Culture Industry Threat**: An article from the North American Journal of Fisheries Management published in February 2023 discusses how decades of global sturgeon conservation efforts are threatened by an expanding captive culture industry.\n\n2. **Lake Sturgeon Endangered**: Great Lakes Now reported in February 2023 that lake sturgeon have been added to the endangered list. However, there's optimism due to various conservation programs, including educational programs by the Little Traverse Bay Band of Odawa Indians, which involve caring for and releasing sturgeon into the wild.\n\n3. **North American Sturgeon and Paddlefish Society (NASPS)**: NASPS aims to promote the conservation and restoration of sturgeon species in North America by advancing research on their biology, management, and utilization. They hold meetings and symposiums, the latest being a symposium at the American Fisheries Society (AFS) 2023 meeting.\n\n4. **Research and Population Assessments**: The U.S. Geological Survey has published papers reviewing the biology, fisheries, and conservation status of Atlantic sturgeon, providing insights into their population and distribution along the U.S. Atlantic Coast.\n\n5. **Regulatory and Conservation Efforts**: There are ongoing efforts by organizations such as the Wildlife Conservation Society of Canada and the U.S. Fish and Wildlife Service to protect sturgeon species. These efforts include endangered species listings, field research, and public engagement in conservation activities.\n\nThese points highlight the importance of continued research funding, understanding of habitat and distribution, societal involvement, awareness of conservation challenges, and endangered species listings in the efforts to protect North American sturgeons.")]}}
---
{'supervisor': {'next': 'Paper writing team'}}
---
{'Paper writing team': {'messages': [HumanMessage(content='Based on the information retrieved from the search results, the following updates and insights on the conservation status of North American sturgeons in 2023 can be provided:\n\n1. **Atlantic Sturgeon**: NOAA Fisheries has updated information on the Atlantic sturgeon as of June 7, 2023, including several 5-year reviews for different Distinct Population Segments (DPS) such as the South Atlantic, Carolina, and New York Bight. These reviews are conducted to ensure that the listing classifications under the Endangered Species Act (ESA) remain accurate.\n\n2. **Bycatch Reduction**: An action plan has been developed to reduce the bycatch of Atlantic sturgeon in federal large mesh gillnet fisheries. This plan is part of the efforts to mitigate one of the significant threats to the sturgeon populations.\n\n3. **Critical Habitat and Recovery Efforts**: Critical habitat maps and GIS data are available for the Atlantic sturgeon, which are protected under the ESA. There is an interim guidance document directing recovery efforts for the endangered New York Bight, Chesapeake Bay, Carolina, and South Atlantic DPSs of the Atlantic sturgeon, as well as the threatened Gulf of Maine DPS, until a full recovery plan is approved.\n\n4. **Outreach & Education**: NOAA Fisheries supports outreach and education through initiatives like SCUTES (Students Collaborating to Undertake Tracking Efforts for Sturgeon), which provides lesson plans to educate students about Atlantic and shortnose sturgeon.\n\n5. **Research & Data**: NOAA Fisheries continues to conduct and support research on the biology, behavior, and ecology of the Atlantic sturgeon to inform conservation and recovery efforts.\n\nThe search results confirm and expand upon the points provided in the initial summary, highlighting the ongoing challenges and efforts in the preservation of North American sturgeons, particularly the Atlantic sturgeon. There is a focus on updating population reviews, reducing bycatch, identifying critical habitats, and promoting educational programs to engage the public in conservation efforts.', name='Search')]}}
---
{'supervisor': {'next': 'Paper writing team'}}
---
{'Paper writing team': {'messages': [HumanMessage(content='Based on the information retrieved from the search results, the following updates and insights on the conservation status of North American sturgeons in 2023 can be provided:\n\n1. **Atlantic Sturgeon**: NOAA Fisheries has updated information on the Atlantic sturgeon as of June 7, 2023, including several 5-year reviews for different Distinct Population Segments (DPS) such as the South Atlantic, Carolina, and New York Bight. These reviews are conducted to ensure that the listing classifications under the Endangered Species Act (ESA) remain accurate.\n\n2. **Bycatch Reduction**: An action plan has been developed to reduce the bycatch of Atlantic sturgeon in federal large mesh gillnet fisheries. This plan is part of the efforts to mitigate one of the significant threats to the sturgeon populations.\n\n3. **Critical Habitat and Recovery Efforts**: Critical habitat maps and GIS data are available for the Atlantic sturgeon, which are protected under the ESA. There is an interim guidance document directing recovery efforts for the endangered New York Bight, Chesapeake Bay, Carolina, and South Atlantic DPSs of the Atlantic sturgeon, as well as the threatened Gulf of Maine DPS, until a full recovery plan is approved.\n\n4. **Outreach & Education**: NOAA Fisheries supports outreach and education through initiatives like SCUTES (Students Collaborating to Undertake Tracking Efforts for Sturgeon), which provides lesson plans to educate students about Atlantic and shortnose sturgeon.\n\n5. **Research & Data**: NOAA Fisheries continues to conduct and support research on the biology, behavior, and ecology of the Atlantic sturgeon to inform conservation and recovery efforts.\n\nThe search results confirm and expand upon the points provided in the initial summary, highlighting the ongoing challenges and efforts in the preservation of North American sturgeons, particularly the Atlantic sturgeon. There is a focus on updating population reviews, reducing bycatch, identifying critical habitats, and promoting educational programs to engage the public in conservation efforts.')]}}
---
{'supervisor': {'next': 'Research team'}}
---
{'Research team': {'messages': [HumanMessage(content='Based on the information retrieved from the search results, the following updates and insights on the conservation status of North American sturgeons in 2023 can be provided:\n\n1. **Atlantic Sturgeon**: NOAA Fisheries has updated information on the Atlantic sturgeon as of June 7, 2023, including several 5-year reviews for different Distinct Population Segments (DPS) such as the South Atlantic, Carolina, and New York Bight. These reviews are conducted to ensure that the listing classifications under the Endangered Species Act (ESA) remain accurate.\n\n2. **Bycatch Reduction**: An action plan has been developed to reduce the bycatch of Atlantic sturgeon in federal large mesh gillnet fisheries. This plan is part of the efforts to mitigate one of the significant threats to the sturgeon populations.\n\n3. **Critical Habitat and Recovery Efforts**: Critical habitat maps and GIS data are available for the Atlantic sturgeon, which are protected under the ESA. There is an interim guidance document directing recovery efforts for the endangered New York Bight, Chesapeake Bay, Carolina, and South Atlantic DPSs of the Atlantic sturgeon, as well as the threatened Gulf of Maine DPS, until a full recovery plan is approved.\n\n4. **Outreach & Education**: NOAA Fisheries supports outreach and education through initiatives like SCUTES (Students Collaborating to Undertake Tracking Efforts for Sturgeon), which provides lesson plans to educate students about Atlantic and shortnose sturgeon.\n\n5. **Research & Data**: NOAA Fisheries continues to conduct and support research on the biology, behavior, and ecology of the Atlantic sturgeon to inform conservation and recovery efforts.\n\nThe search results confirm and expand upon the points provided in the initial summary, highlighting the ongoing challenges and efforts in the preservation of North American sturgeons, particularly the Atlantic sturgeon. There is a focus on updating population reviews, reducing bycatch, identifying critical habitats, and promoting educational programs to engage the public in conservation efforts.')]}}
---
{'supervisor': {'next': 'Paper writing team'}}
---
{'Paper writing team': {'messages': [HumanMessage(content='Based on the information retrieved from the search results, the following updates and insights on the conservation status of North American sturgeons in 2023 can be provided:\n\n1. **Atlantic Sturgeon**: NOAA Fisheries has updated information on the Atlantic sturgeon as of June 7, 2023, including several 5-year reviews for different Distinct Population Segments (DPS) such as the South Atlantic, Carolina, and New York Bight. These reviews are conducted to ensure that the listing classifications under the Endangered Species Act (ESA) remain accurate.\n\n2. **Bycatch Reduction**: An action plan has been developed to reduce the bycatch of Atlantic sturgeon in federal large mesh gillnet fisheries. This plan is part of the efforts to mitigate one of the significant threats to the sturgeon populations.\n\n3. **Critical Habitat and Recovery Efforts**: Critical habitat maps and GIS data are available for the Atlantic sturgeon, which are protected under the ESA. There is an interim guidance document directing recovery efforts for the endangered New York Bight, Chesapeake Bay, Carolina, and South Atlantic DPSs of the Atlantic sturgeon, as well as the threatened Gulf of Maine DPS, until a full recovery plan is approved.\n\n4. **Outreach & Education**: NOAA Fisheries supports outreach and education through initiatives like SCUTES (Students Collaborating to Undertake Tracking Efforts for Sturgeon), which provides lesson plans to educate students about Atlantic and shortnose sturgeon.\n\n5. **Research & Data**: NOAA Fisheries continues to conduct and support research on the biology, behavior, and ecology of the Atlantic sturgeon to inform conservation and recovery efforts.\n\nThe search results confirm and expand upon the points provided in the initial summary, highlighting the ongoing challenges and efforts in the preservation of North American sturgeons, particularly the Atlantic sturgeon. There is a focus on updating population reviews, reducing bycatch, identifying critical habitats, and promoting educational programs to engage the public in conservation efforts.')]}}
---
{'supervisor': {'next': 'Research team'}}
---
{'Research team': {'messages': [HumanMessage(content='Based on the information retrieved from the search results, the following updates and insights on the conservation status of North American sturgeons in 2023 can be provided:\n\n1. **Atlantic Sturgeon**: NOAA Fisheries has updated information on the Atlantic sturgeon as of June 7, 2023, including several 5-year reviews for different Distinct Population Segments (DPS) such as the South Atlantic, Carolina, and New York Bight. These reviews are conducted to ensure that the listing classifications under the Endangered Species Act (ESA) remain accurate.\n\n2. **Bycatch Reduction**: An action plan has been developed to reduce the bycatch of Atlantic sturgeon in federal large mesh gillnet fisheries. This plan is part of the efforts to mitigate one of the significant threats to the sturgeon populations.\n\n3. **Critical Habitat and Recovery Efforts**: Critical habitat maps and GIS data are available for the Atlantic sturgeon, which are protected under the ESA. There is an interim guidance document directing recovery efforts for the endangered New York Bight, Chesapeake Bay, Carolina, and South Atlantic DPSs of the Atlantic sturgeon, as well as the threatened Gulf of Maine DPS, until a full recovery plan is approved.\n\n4. **Outreach & Education**: NOAA Fisheries supports outreach and education through initiatives like SCUTES (Students Collaborating to Undertake Tracking Efforts for Sturgeon), which provides lesson plans to educate students about Atlantic and shortnose sturgeon.\n\n5. **Research & Data**: NOAA Fisheries continues to conduct and support research on the biology, behavior, and ecology of the Atlantic sturgeon to inform conservation and recovery efforts.\n\nThe search results confirm and expand upon the points provided in the initial summary, highlighting the ongoing challenges and efforts in the preservation of North American sturgeons, particularly the Atlantic sturgeon. There is a focus on updating population reviews, reducing bycatch, identifying critical habitats, and promoting educational programs to engage the public in conservation efforts.')]}}
---
{'supervisor': {'next': 'Paper writing team'}}
---
{'Paper writing team': {'messages': [HumanMessage(content='Based on the information retrieved from the search results, the following updates and insights on the conservation status of North American sturgeons in 2023 can be provided:\n\n1. **Atlantic Sturgeon**: NOAA Fisheries has updated information on the Atlantic sturgeon as of June 7, 2023, including several 5-year reviews for different Distinct Population Segments (DPS) such as the South Atlantic, Carolina, and New York Bight. These reviews are conducted to ensure that the listing classifications under the Endangered Species Act (ESA) remain accurate.\n\n2. **Bycatch Reduction**: An action plan has been developed to reduce the bycatch of Atlantic sturgeon in federal large mesh gillnet fisheries. This plan is part of the efforts to mitigate one of the significant threats to the sturgeon populations.\n\n3. **Critical Habitat and Recovery Efforts**: Critical habitat maps and GIS data are available for the Atlantic sturgeon, which are protected under the ESA. There is an interim guidance document directing recovery efforts for the endangered New York Bight, Chesapeake Bay, Carolina, and South Atlantic DPSs of the Atlantic sturgeon, as well as the threatened Gulf of Maine DPS, until a full recovery plan is approved.\n\n4. **Outreach & Education**: NOAA Fisheries supports outreach and education through initiatives like SCUTES (Students Collaborating to Undertake Tracking Efforts for Sturgeon), which provides lesson plans to educate students about Atlantic and shortnose sturgeon.\n\n5. **Research & Data**: NOAA Fisheries continues to conduct and support research on the biology, behavior, and ecology of the Atlantic sturgeon to inform conservation and recovery efforts.\n\nThe search results confirm and expand upon the points provided in the initial summary, highlighting the ongoing challenges and efforts in the preservation of North American sturgeons, particularly the Atlantic sturgeon. There is a focus on updating population reviews, reducing bycatch, identifying critical habitats, and promoting educational programs to engage the public in conservation efforts.')]}}
---
{'supervisor': {'next': 'Paper writing team'}}
---
{'Paper writing team': {'messages': [HumanMessage(content='Based on the information retrieved from the search results, the following updates and insights on the conservation status of North American sturgeons in 2023 can be provided:\n\n1. **Atlantic Sturgeon**: NOAA Fisheries has updated information on the Atlantic sturgeon as of June 7, 2023, including several 5-year reviews for different Distinct Population Segments (DPS) such as the South Atlantic, Carolina, and New York Bight. These reviews are conducted to ensure that the listing classifications under the Endangered Species Act (ESA) remain accurate.\n\n2. **Bycatch Reduction**: An action plan has been developed to reduce the bycatch of Atlantic sturgeon in federal large mesh gillnet fisheries. This plan is part of the efforts to mitigate one of the significant threats to the sturgeon populations.\n\n3. **Critical Habitat and Recovery Efforts**: Critical habitat maps and GIS data are available for the Atlantic sturgeon, which are protected under the ESA. There is an interim guidance document directing recovery efforts for the endangered New York Bight, Chesapeake Bay, Carolina, and South Atlantic DPSs of the Atlantic sturgeon, as well as the threatened Gulf of Maine DPS, until a full recovery plan is approved.\n\n4. **Outreach & Education**: NOAA Fisheries supports outreach and education through initiatives like SCUTES (Students Collaborating to Undertake Tracking Efforts for Sturgeon), which provides lesson plans to educate students about Atlantic and shortnose sturgeon.\n\n5. **Research & Data**: NOAA Fisheries continues to conduct and support research on the biology, behavior, and ecology of the Atlantic sturgeon to inform conservation and recovery efforts.\n\nThe search results confirm and expand upon the points provided in the initial summary, highlighting the ongoing challenges and efforts in the preservation of North American sturgeons, particularly the Atlantic sturgeon. There is a focus on updating population reviews, reducing bycatch, identifying critical habitats, and promoting educational programs to engage the public in conservation efforts.')]}}
---
{'supervisor': {'next': 'Research team'}}
---
{'Research team': {'messages': [HumanMessage(content='Based on the information retrieved from the search results, the following updates and insights on the conservation status of North American sturgeons in 2023 can be provided:\n\n1. **Atlantic Sturgeon**: NOAA Fisheries has updated information on the Atlantic sturgeon as of June 7, 2023, including several 5-year reviews for different Distinct Population Segments (DPS) such as the South Atlantic, Carolina, and New York Bight. These reviews are conducted to ensure that the listing classifications under the Endangered Species Act (ESA) remain accurate.\n\n2. **Bycatch Reduction**: An action plan has been developed to reduce the bycatch of Atlantic sturgeon in federal large mesh gillnet fisheries. This plan is part of the efforts to mitigate one of the significant threats to the sturgeon populations.\n\n3. **Critical Habitat and Recovery Efforts**: Critical habitat maps and GIS data are available for the Atlantic sturgeon, which are protected under the ESA. There is an interim guidance document directing recovery efforts for the endangered New York Bight, Chesapeake Bay, Carolina, and South Atlantic DPSs of the Atlantic sturgeon, as well as the threatened Gulf of Maine DPS, until a full recovery plan is approved.\n\n4. **Outreach & Education**: NOAA Fisheries supports outreach and education through initiatives like SCUTES (Students Collaborating to Undertake Tracking Efforts for Sturgeon), which provides lesson plans to educate students about Atlantic and shortnose sturgeon.\n\n5. **Research & Data**: NOAA Fisheries continues to conduct and support research on the biology, behavior, and ecology of the Atlantic sturgeon to inform conservation and recovery efforts.\n\nThe search results confirm and expand upon the points provided in the initial summary, highlighting the ongoing challenges and efforts in the preservation of North American sturgeons, particularly the Atlantic sturgeon. There is a focus on updating population reviews, reducing bycatch, identifying critical habitats, and promoting educational programs to engage the public in conservation efforts.')]}}
---
{'supervisor': {'next': 'Paper writing team'}}
---
{'Paper writing team': {'messages': [HumanMessage(content='Based on the information retrieved from the search results, the following updates and insights on the conservation status of North American sturgeons in 2023 can be provided:\n\n1. **Atlantic Sturgeon**: NOAA Fisheries has updated information on the Atlantic sturgeon as of June 7, 2023, including several 5-year reviews for different Distinct Population Segments (DPS) such as the South Atlantic, Carolina, and New York Bight. These reviews are conducted to ensure that the listing classifications under the Endangered Species Act (ESA) remain accurate.\n\n2. **Bycatch Reduction**: An action plan has been developed to reduce the bycatch of Atlantic sturgeon in federal large mesh gillnet fisheries. This plan is part of the efforts to mitigate one of the significant threats to the sturgeon populations.\n\n3. **Critical Habitat and Recovery Efforts**: Critical habitat maps and GIS data are available for the Atlantic sturgeon, which are protected under the ESA. There is an interim guidance document directing recovery efforts for the endangered New York Bight, Chesapeake Bay, Carolina, and South Atlantic DPSs of the Atlantic sturgeon, as well as the threatened Gulf of Maine DPS, until a full recovery plan is approved.\n\n4. **Outreach & Education**: NOAA Fisheries supports outreach and education through initiatives like SCUTES (Students Collaborating to Undertake Tracking Efforts for Sturgeon), which provides lesson plans to educate students about Atlantic and shortnose sturgeon.\n\n5. **Research & Data**: NOAA Fisheries continues to conduct and support research on the biology, behavior, and ecology of the Atlantic sturgeon to inform conservation and recovery efforts.\n\nThe search results confirm and expand upon the points provided in the initial summary, highlighting the ongoing challenges and efforts in the preservation of North American sturgeons, particularly the Atlantic sturgeon. There is a focus on updating population reviews, reducing bycatch, identifying critical habitats, and promoting educational programs to engage the public in conservation efforts.')]}}
---
{'supervisor': {'next': 'Paper writing team'}}
---
{'Paper writing team': {'messages': [HumanMessage(content='Based on the information retrieved from the search results, the following updates and insights on the conservation status of North American sturgeons in 2023 can be provided:\n\n1. **Atlantic Sturgeon**: NOAA Fisheries has updated information on the Atlantic sturgeon as of June 7, 2023, including several 5-year reviews for different Distinct Population Segments (DPS) such as the South Atlantic, Carolina, and New York Bight. These reviews are conducted to ensure that the listing classifications under the Endangered Species Act (ESA) remain accurate.\n\n2. **Bycatch Reduction**: An action plan has been developed to reduce the bycatch of Atlantic sturgeon in federal large mesh gillnet fisheries. This plan is part of the efforts to mitigate one of the significant threats to the sturgeon populations.\n\n3. **Critical Habitat and Recovery Efforts**: Critical habitat maps and GIS data are available for the Atlantic sturgeon, which are protected under the ESA. There is an interim guidance document directing recovery efforts for the endangered New York Bight, Chesapeake Bay, Carolina, and South Atlantic DPSs of the Atlantic sturgeon, as well as the threatened Gulf of Maine DPS, until a full recovery plan is approved.\n\n4. **Outreach & Education**: NOAA Fisheries supports outreach and education through initiatives like SCUTES (Students Collaborating to Undertake Tracking Efforts for Sturgeon), which provides lesson plans to educate students about Atlantic and shortnose sturgeon.\n\n5. **Research & Data**: NOAA Fisheries continues to conduct and support research on the biology, behavior, and ecology of the Atlantic sturgeon to inform conservation and recovery efforts.\n\nThe search results confirm and expand upon the points provided in the initial summary, highlighting the ongoing challenges and efforts in the preservation of North American sturgeons, particularly the Atlantic sturgeon. There is a focus on updating population reviews, reducing bycatch, identifying critical habitats, and promoting educational programs to engage the public in conservation efforts.')]}}
---
{'supervisor': {'next': 'Research team'}}
---
{'Research team': {'messages': [HumanMessage(content='Based on the information retrieved from the search results, the following updates and insights on the conservation status of North American sturgeons in 2023 can be provided:\n\n1. **Atlantic Sturgeon**: NOAA Fisheries has updated information on the Atlantic sturgeon as of June 7, 2023, including several 5-year reviews for different Distinct Population Segments (DPS) such as the South Atlantic, Carolina, and New York Bight. These reviews are conducted to ensure that the listing classifications under the Endangered Species Act (ESA) remain accurate.\n\n2. **Bycatch Reduction**: An action plan has been developed to reduce the bycatch of Atlantic sturgeon in federal large mesh gillnet fisheries. This plan is part of the efforts to mitigate one of the significant threats to the sturgeon populations.\n\n3. **Critical Habitat and Recovery Efforts**: Critical habitat maps and GIS data are available for the Atlantic sturgeon, which are protected under the ESA. There is an interim guidance document directing recovery efforts for the endangered New York Bight, Chesapeake Bay, Carolina, and South Atlantic DPSs of the Atlantic sturgeon, as well as the threatened Gulf of Maine DPS, until a full recovery plan is approved.\n\n4. **Outreach & Education**: NOAA Fisheries supports outreach and education through initiatives like SCUTES (Students Collaborating to Undertake Tracking Efforts for Sturgeon), which provides lesson plans to educate students about Atlantic and shortnose sturgeon.\n\n5. **Research & Data**: NOAA Fisheries continues to conduct and support research on the biology, behavior, and ecology of the Atlantic sturgeon to inform conservation and recovery efforts.\n\nThe search results confirm and expand upon the points provided in the initial summary, highlighting the ongoing challenges and efforts in the preservation of North American sturgeons, particularly the Atlantic sturgeon. There is a focus on updating population reviews, reducing bycatch, identifying critical habitats, and promoting educational programs to engage the public in conservation efforts.')]}}
---
{'supervisor': {'next': 'Research team'}}
---
{'Research team': {'messages': [HumanMessage(content='Based on the information retrieved from the search results, the following updates and insights on the conservation status of North American sturgeons in 2023 can be provided:\n\n1. **Atlantic Sturgeon**: NOAA Fisheries has updated information on the Atlantic sturgeon as of June 7, 2023, including several 5-year reviews for different Distinct Population Segments (DPS) such as the South Atlantic, Carolina, and New York Bight. These reviews are conducted to ensure that the listing classifications under the Endangered Species Act (ESA) remain accurate.\n\n2. **Bycatch Reduction**: An action plan has been developed to reduce the bycatch of Atlantic sturgeon in federal large mesh gillnet fisheries. This plan is part of the efforts to mitigate one of the significant threats to the sturgeon populations.\n\n3. **Critical Habitat and Recovery Efforts**: Critical habitat maps and GIS data are available for the Atlantic sturgeon, which are protected under the ESA. There is an interim guidance document directing recovery efforts for the endangered New York Bight, Chesapeake Bay, Carolina, and South Atlantic DPSs of the Atlantic sturgeon, as well as the threatened Gulf of Maine DPS, until a full recovery plan is approved.\n\n4. **Outreach & Education**: NOAA Fisheries supports outreach and education through initiatives like SCUTES (Students Collaborating to Undertake Tracking Efforts for Sturgeon), which provides lesson plans to educate students about Atlantic and shortnose sturgeon.\n\n5. **Research & Data**: NOAA Fisheries continues to conduct and support research on the biology, behavior, and ecology of the Atlantic sturgeon to inform conservation and recovery efforts.\n\nThe search results confirm and expand upon the points provided in the initial summary, highlighting the ongoing challenges and efforts in the preservation of North American sturgeons, particularly the Atlantic sturgeon. There is a focus on updating population reviews, reducing bycatch, identifying critical habitats, and promoting educational programs to engage the public in conservation efforts.')]}}
---
{'supervisor': {'next': 'FINISH'}}
---

%pip freeze > ./requirements.txt


     
Note: you may need to restart the kernel to use updated packages.








##### FILE: agent_supervisor.ipynb #####


# Multi-agent supervisor



The [previous example](../multi-agent-collaboration) routed messages automatically based on the output of the initial researcher agent.



We can also choose to use an [LLM to orchestrate](https://langchain-ai.github.io/langgraph/concepts/multi_agent/#supervisor) the different agents.



Below, we will create an agent group, with an agent supervisor to help delegate tasks.



![diagram](attachment:8ee0a8ce-f0a8-4019-b5bf-b20933e40956.png)



To simplify the code in each agent node, we will use LangGraph's prebuilt [create_react_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent). This and other "advanced agent" notebooks are designed to show how you can implement certain design patterns in LangGraph. If the pattern suits your needs, we recommend combining it with some of the other fundamental patterns described elsewhere in the docs for best performance.



## Setup



First, let's install required packages and set our API keys


%%capture --no-stderr

%pip install -U langgraph langchain_community langchain_anthropic langchain_experimental


import getpass

import os





def _set_if_undefined(var: str):

    if not os.environ.get(var):

        os.environ[var] = getpass.getpass(f"Please provide your {var}")





_set_if_undefined("ANTHROPIC_API_KEY")

_set_if_undefined("TAVILY_API_KEY")


<div class="admonition tip">

    <p class="admonition-title">Set up <a href="https://smith.langchain.com">LangSmith</a> for LangGraph development</p>

    <p style="padding-top: 5px;">

        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph  read more about how to get started <a href="https://docs.smith.langchain.com">here</a>. 

    </p>

</div>


## Create tools



For this example, you will make an agent to do web research with a search engine, and one agent to create plots. Define the tools they'll use below:


from typing import Annotated



from langchain_community.tools.tavily_search import TavilySearchResults

from langchain_core.tools import tool

from langchain_experimental.utilities import PythonREPL



tavily_tool = TavilySearchResults(max_results=5)



# This executes code locally, which can be unsafe

repl = PythonREPL()





@tool

def python_repl_tool(

    code: Annotated[str, "The python code to execute to generate your chart."],

):

    """Use this to execute python code and do math. If you want to see the output of a value,

    you should print it out with `print(...)`. This is visible to the user."""

    try:

        result = repl.run(code)

    except BaseException as e:

        return f"Failed to execute. Error: {repr(e)}"

    result_str = f"Successfully executed:\n```python\n{code}\n```\nStdout: {result}"

    return result_str


### Create Agent Supervisor



It will use LLM with structured output to choose the next worker node OR finish processing.


from typing import Literal

from typing_extensions import TypedDict



from langchain_anthropic import ChatAnthropic

from langgraph.graph import MessagesState, END

from langgraph.types import Command





members = ["researcher", "coder"]

# Our team supervisor is an LLM node. It just picks the next agent to process

# and decides when the work is completed

options = members + ["FINISH"]



system_prompt = (

    "You are a supervisor tasked with managing a conversation between the"

    f" following workers: {members}. Given the following user request,"

    " respond with the worker to act next. Each worker will perform a"

    " task and respond with their results and status. When finished,"

    " respond with FINISH."

)





class Router(TypedDict):

    """Worker to route to next. If no workers needed, route to FINISH."""



    next: Literal[*options]





llm = ChatAnthropic(model="claude-3-5-sonnet-latest")





class State(MessagesState):

    next: str





def supervisor_node(state: State) -> Command[Literal[*members, "__end__"]]:

    messages = [

        {"role": "system", "content": system_prompt},

    ] + state["messages"]

    response = llm.with_structured_output(Router).invoke(messages)

    goto = response["next"]

    if goto == "FINISH":

        goto = END



    return Command(goto=goto, update={"next": goto})


## Construct Graph



We're ready to start building the graph. Below, define the state and worker nodes using the function we just defined.


from langchain_core.messages import HumanMessage

from langgraph.graph import StateGraph, START, END

from langgraph.prebuilt import create_react_agent





research_agent = create_react_agent(

    llm, tools=[tavily_tool], prompt="You are a researcher. DO NOT do any math."

)





def research_node(state: State) -> Command[Literal["supervisor"]]:

    result = research_agent.invoke(state)

    return Command(

        update={

            "messages": [

                HumanMessage(content=result["messages"][-1].content, name="researcher")

            ]

        },

        goto="supervisor",

    )





# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED

code_agent = create_react_agent(llm, tools=[python_repl_tool])





def code_node(state: State) -> Command[Literal["supervisor"]]:

    result = code_agent.invoke(state)

    return Command(

        update={

            "messages": [

                HumanMessage(content=result["messages"][-1].content, name="coder")

            ]

        },

        goto="supervisor",

    )





builder = StateGraph(State)

builder.add_edge(START, "supervisor")

builder.add_node("supervisor", supervisor_node)

builder.add_node("researcher", research_node)

builder.add_node("coder", code_node)

graph = builder.compile()


from IPython.display import display, Image



display(Image(graph.get_graph().draw_mermaid_png()))


## Invoke the team



With the graph created, we can now invoke it and see how it performs!


for s in graph.stream(

    {"messages": [("user", "What's the square root of 42?")]}, subgraphs=True

):

    print(s)

    print("----")


for s in graph.stream(

    {

        "messages": [

            (

                "user",

                "Find the latest GDP of New York and California, then calculate the average",

            )

        ]

    },

    subgraphs=True,

):

    print(s)

    print("----")




##### FILE: hierarchical_agent_teams.ipynb #####


# Hierarchical Agent Teams



In our previous example ([Agent Supervisor](../agent_supervisor)), we introduced the concept of a single [supervisor node](https://langchain-ai.github.io/langgraph/concepts/multi_agent/#supervisor) to route work between different worker nodes.



But what if the job for a single worker becomes too complex? What if the number of workers becomes too large?



For some applications, the system may be more effective if work is distributed _hierarchically_.



You can do this by composing different subgraphs and creating a top-level supervisor, along with mid-level supervisors.



To do this, let's build a simple research assistant! The graph will look something like the following:



![diagram](attachment:d98ed25c-51cb-441f-a6f4-016921d59fc3.png)



This notebook is inspired by the paper [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/abs/2308.08155), by Wu, et. al. In the rest of this notebook, you will:



1. Define the agents' tools to access the web and write files

2. Define some utilities to help create the graph and agents

3. Create and define each team (web research + doc writing)

4. Compose everything together.



## Setup



First, let's install our required packages and set our API keys


%%capture --no-stderr

%pip install -U langgraph langchain_community langchain_anthropic langchain_experimental


import getpass

import os





def _set_if_undefined(var: str):

    if not os.environ.get(var):

        os.environ[var] = getpass.getpass(f"Please provide your {var}")





_set_if_undefined("OPENAI_API_KEY")

_set_if_undefined("TAVILY_API_KEY")


<div class="admonition tip">

    <p class="admonition-title">Set up <a href="https://smith.langchain.com">LangSmith</a> for LangGraph development</p>

    <p style="padding-top: 5px;">

        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph  read more about how to get started <a href="https://docs.smith.langchain.com">here</a>. 

    </p>

</div>


## Create Tools



Each team will be composed of one or more agents each with one or more tools. Below, define all the tools to be used by your different teams.



We'll start with the research team.



**ResearchTeam tools**



The research team can use a search engine and url scraper to find information on the web. Feel free to add additional functionality below to boost the team performance!


from typing import Annotated, List



from langchain_community.document_loaders import WebBaseLoader

from langchain_community.tools.tavily_search import TavilySearchResults

from langchain_core.tools import tool



tavily_tool = TavilySearchResults(max_results=5)





@tool

def scrape_webpages(urls: List[str]) -> str:

    """Use requests and bs4 to scrape the provided web pages for detailed information."""

    loader = WebBaseLoader(urls)

    docs = loader.load()

    return "\n\n".join(

        [

            f'<Document name="{doc.metadata.get("title", "")}">\n{doc.page_content}\n</Document>'

            for doc in docs

        ]

    )


**Document writing team tools**



Next up, we will give some tools for the doc writing team to use.

We define some bare-bones file-access tools below.



Note that this gives the agents access to your file-system, which can be unsafe. We also haven't optimized the tool descriptions for performance.


from pathlib import Path

from tempfile import TemporaryDirectory

from typing import Dict, Optional



from langchain_experimental.utilities import PythonREPL

from typing_extensions import TypedDict



_TEMP_DIRECTORY = TemporaryDirectory()

WORKING_DIRECTORY = Path(_TEMP_DIRECTORY.name)





@tool

def create_outline(

    points: Annotated[List[str], "List of main points or sections."],

    file_name: Annotated[str, "File path to save the outline."],

) -> Annotated[str, "Path of the saved outline file."]:

    """Create and save an outline."""

    with (WORKING_DIRECTORY / file_name).open("w") as file:

        for i, point in enumerate(points):

            file.write(f"{i + 1}. {point}\n")

    return f"Outline saved to {file_name}"





@tool

def read_document(

    file_name: Annotated[str, "File path to read the document from."],

    start: Annotated[Optional[int], "The start line. Default is 0"] = None,

    end: Annotated[Optional[int], "The end line. Default is None"] = None,

) -> str:

    """Read the specified document."""

    with (WORKING_DIRECTORY / file_name).open("r") as file:

        lines = file.readlines()

    if start is None:

        start = 0

    return "\n".join(lines[start:end])





@tool

def write_document(

    content: Annotated[str, "Text content to be written into the document."],

    file_name: Annotated[str, "File path to save the document."],

) -> Annotated[str, "Path of the saved document file."]:

    """Create and save a text document."""

    with (WORKING_DIRECTORY / file_name).open("w") as file:

        file.write(content)

    return f"Document saved to {file_name}"





@tool

def edit_document(

    file_name: Annotated[str, "Path of the document to be edited."],

    inserts: Annotated[

        Dict[int, str],

        "Dictionary where key is the line number (1-indexed) and value is the text to be inserted at that line.",

    ],

) -> Annotated[str, "Path of the edited document file."]:

    """Edit a document by inserting text at specific line numbers."""



    with (WORKING_DIRECTORY / file_name).open("r") as file:

        lines = file.readlines()



    sorted_inserts = sorted(inserts.items())



    for line_number, text in sorted_inserts:

        if 1 <= line_number <= len(lines) + 1:

            lines.insert(line_number - 1, text + "\n")

        else:

            return f"Error: Line number {line_number} is out of range."



    with (WORKING_DIRECTORY / file_name).open("w") as file:

        file.writelines(lines)



    return f"Document edited and saved to {file_name}"





# Warning: This executes code locally, which can be unsafe when not sandboxed



repl = PythonREPL()





@tool

def python_repl_tool(

    code: Annotated[str, "The python code to execute to generate your chart."],

):

    """Use this to execute python code. If you want to see the output of a value,

    you should print it out with `print(...)`. This is visible to the user."""

    try:

        result = repl.run(code)

    except BaseException as e:

        return f"Failed to execute. Error: {repr(e)}"

    return f"Successfully executed:\n```python\n{code}\n```\nStdout: {result}"


## Helper Utilities



We are going to create a few utility functions to make it more concise when we want to:



1. Create a worker agent.

2. Create a supervisor for the sub-graph.



These will simplify the graph compositional code at the end for us so it's easier to see what's going on.


from typing import List, Optional, Literal

from langchain_core.language_models.chat_models import BaseChatModel



from langgraph.graph import StateGraph, MessagesState, START, END

from langgraph.types import Command

from langchain_core.messages import HumanMessage, trim_messages





class State(MessagesState):

    next: str





def make_supervisor_node(llm: BaseChatModel, members: list[str]) -> str:

    options = ["FINISH"] + members

    system_prompt = (

        "You are a supervisor tasked with managing a conversation between the"

        f" following workers: {members}. Given the following user request,"

        " respond with the worker to act next. Each worker will perform a"

        " task and respond with their results and status. When finished,"

        " respond with FINISH."

    )



    class Router(TypedDict):

        """Worker to route to next. If no workers needed, route to FINISH."""



        next: Literal[*options]



    def supervisor_node(state: State) -> Command[Literal[*members, "__end__"]]:

        """An LLM-based router."""

        messages = [

            {"role": "system", "content": system_prompt},

        ] + state["messages"]

        response = llm.with_structured_output(Router).invoke(messages)

        goto = response["next"]

        if goto == "FINISH":

            goto = END



        return Command(goto=goto, update={"next": goto})



    return supervisor_node


## Define Agent Teams



Now we can get to define our hierarchical teams. "Choose your player!"



### Research Team



The research team will have a search agent and a web scraping "research_agent" as the two worker nodes. Let's create those, as well as the team supervisor.


from langchain_core.messages import HumanMessage

from langchain_openai import ChatOpenAI

from langgraph.prebuilt import create_react_agent



llm = ChatOpenAI(model="gpt-4o")



search_agent = create_react_agent(llm, tools=[tavily_tool])





def search_node(state: State) -> Command[Literal["supervisor"]]:

    result = search_agent.invoke(state)

    return Command(

        update={

            "messages": [

                HumanMessage(content=result["messages"][-1].content, name="search")

            ]

        },

        # We want our workers to ALWAYS "report back" to the supervisor when done

        goto="supervisor",

    )





web_scraper_agent = create_react_agent(llm, tools=[scrape_webpages])





def web_scraper_node(state: State) -> Command[Literal["supervisor"]]:

    result = web_scraper_agent.invoke(state)

    return Command(

        update={

            "messages": [

                HumanMessage(content=result["messages"][-1].content, name="web_scraper")

            ]

        },

        # We want our workers to ALWAYS "report back" to the supervisor when done

        goto="supervisor",

    )





research_supervisor_node = make_supervisor_node(llm, ["search", "web_scraper"])


Now that we've created the necessary components, defining their interactions is easy. Add the nodes to the team graph, and define the edges, which determine the transition criteria.


research_builder = StateGraph(State)

research_builder.add_node("supervisor", research_supervisor_node)

research_builder.add_node("search", search_node)

research_builder.add_node("web_scraper", web_scraper_node)



research_builder.add_edge(START, "supervisor")

research_graph = research_builder.compile()


from IPython.display import Image, display



display(Image(research_graph.get_graph().draw_mermaid_png()))


We can give this team work directly. Try it out below.


for s in research_graph.stream(

    {"messages": [("user", "when is Taylor Swift's next tour?")]},

    {"recursion_limit": 100},

):

    print(s)

    print("---")


### Document Writing Team



Create the document writing team below using a similar approach. This time, we will give each agent access to different file-writing tools.



Note that we are giving file-system access to our agent here, which is not safe in all cases.


llm = ChatOpenAI(model="gpt-4o")



doc_writer_agent = create_react_agent(

    llm,

    tools=[write_document, edit_document, read_document],

    prompt=(

        "You can read, write and edit documents based on note-taker's outlines. "

        "Don't ask follow-up questions."

    ),

)





def doc_writing_node(state: State) -> Command[Literal["supervisor"]]:

    result = doc_writer_agent.invoke(state)

    return Command(

        update={

            "messages": [

                HumanMessage(content=result["messages"][-1].content, name="doc_writer")

            ]

        },

        # We want our workers to ALWAYS "report back" to the supervisor when done

        goto="supervisor",

    )





note_taking_agent = create_react_agent(

    llm,

    tools=[create_outline, read_document],

    prompt=(

        "You can read documents and create outlines for the document writer. "

        "Don't ask follow-up questions."

    ),

)





def note_taking_node(state: State) -> Command[Literal["supervisor"]]:

    result = note_taking_agent.invoke(state)

    return Command(

        update={

            "messages": [

                HumanMessage(content=result["messages"][-1].content, name="note_taker")

            ]

        },

        # We want our workers to ALWAYS "report back" to the supervisor when done

        goto="supervisor",

    )





chart_generating_agent = create_react_agent(

    llm, tools=[read_document, python_repl_tool]

)





def chart_generating_node(state: State) -> Command[Literal["supervisor"]]:

    result = chart_generating_agent.invoke(state)

    return Command(

        update={

            "messages": [

                HumanMessage(

                    content=result["messages"][-1].content, name="chart_generator"

                )

            ]

        },

        # We want our workers to ALWAYS "report back" to the supervisor when done

        goto="supervisor",

    )





doc_writing_supervisor_node = make_supervisor_node(

    llm, ["doc_writer", "note_taker", "chart_generator"]

)


With the objects themselves created, we can form the graph.


# Create the graph here

paper_writing_builder = StateGraph(State)

paper_writing_builder.add_node("supervisor", doc_writing_supervisor_node)

paper_writing_builder.add_node("doc_writer", doc_writing_node)

paper_writing_builder.add_node("note_taker", note_taking_node)

paper_writing_builder.add_node("chart_generator", chart_generating_node)



paper_writing_builder.add_edge(START, "supervisor")

paper_writing_graph = paper_writing_builder.compile()


from IPython.display import Image, display



display(Image(paper_writing_graph.get_graph().draw_mermaid_png()))


for s in paper_writing_graph.stream(

    {

        "messages": [

            (

                "user",

                "Write an outline for poem about cats and then write the poem to disk.",

            )

        ]

    },

    {"recursion_limit": 100},

):

    print(s)

    print("---")


## Add Layers



In this design, we are enforcing a top-down planning policy. We've created two graphs already, but we have to decide how to route work between the two.



We'll create a _third_ graph to orchestrate the previous two, and add some connectors to define how this top-level state is shared between the different graphs.


from langchain_core.messages import BaseMessage



llm = ChatOpenAI(model="gpt-4o")



teams_supervisor_node = make_supervisor_node(llm, ["research_team", "writing_team"])


def call_research_team(state: State) -> Command[Literal["supervisor"]]:

    response = research_graph.invoke({"messages": state["messages"][-1]})

    return Command(

        update={

            "messages": [

                HumanMessage(

                    content=response["messages"][-1].content, name="research_team"

                )

            ]

        },

        goto="supervisor",

    )





def call_paper_writing_team(state: State) -> Command[Literal["supervisor"]]:

    response = paper_writing_graph.invoke({"messages": state["messages"][-1]})

    return Command(

        update={

            "messages": [

                HumanMessage(

                    content=response["messages"][-1].content, name="writing_team"

                )

            ]

        },

        goto="supervisor",

    )





# Define the graph.

super_builder = StateGraph(State)

super_builder.add_node("supervisor", teams_supervisor_node)

super_builder.add_node("research_team", call_research_team)

super_builder.add_node("writing_team", call_paper_writing_team)



super_builder.add_edge(START, "supervisor")

super_graph = super_builder.compile()


from IPython.display import Image, display



display(Image(super_graph.get_graph().draw_mermaid_png()))


for s in super_graph.stream(

    {

        "messages": [

            ("user", "Research AI agents and write a brief report about them.")

        ],

    },

    {"recursion_limit": 150},

):

    print(s)

    print("---")




##### FILE: multi-agent-collaboration.ipynb #####


# Multi-agent network



A single agent can usually operate effectively using a handful of tools within a single domain, but even using powerful models like `gpt-4`, it can be less effective at using many tools. 



One way to approach complicated tasks is through a "divide-and-conquer" approach: create a specialized agent for each task or domain and route tasks to the correct "expert". This is an example of a [multi-agent network](https://langchain-ai.github.io/langgraph/concepts/multi_agent/#network) architecture.



This notebook (inspired by the paper [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/abs/2308.08155), by Wu, et. al.) shows one way to do this using LangGraph.



The resulting graph will look something like the following diagram:



![multi_agent diagram](attachment:8088306a-da20-4f95-bb07-c3fbd546762c.png)



Before we get started, a quick note: this and other multi-agent notebooks are designed to show _how_ you can implement certain design patterns in LangGraph. If the pattern suits your needs, we recommend combining it with some of the other fundamental patterns described elsewhere in the docs for best performance.



## Setup



First, let's install our required packages and set our API keys:


%%capture --no-stderr

%pip install -U langchain_community langchain_anthropic langchain_experimental matplotlib langgraph


import getpass

import os





def _set_if_undefined(var: str):

    if not os.environ.get(var):

        os.environ[var] = getpass.getpass(f"Please provide your {var}")





_set_if_undefined("ANTHROPIC_API_KEY")

_set_if_undefined("TAVILY_API_KEY")


<div class="admonition tip">

    <p class="admonition-title">Set up <a href="https://smith.langchain.com">LangSmith</a> for LangGraph development</p>

    <p style="padding-top: 5px;">

        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph  read more about how to get started <a href="https://docs.smith.langchain.com">here</a>. 

    </p>

</div>


## Define tools



We will also define some tools that our agents will use in the future


from typing import Annotated



from langchain_community.tools.tavily_search import TavilySearchResults

from langchain_core.tools import tool

from langchain_experimental.utilities import PythonREPL



tavily_tool = TavilySearchResults(max_results=5)



# Warning: This executes code locally, which can be unsafe when not sandboxed



repl = PythonREPL()





@tool

def python_repl_tool(

    code: Annotated[str, "The python code to execute to generate your chart."],

):

    """Use this to execute python code. If you want to see the output of a value,

    you should print it out with `print(...)`. This is visible to the user."""

    try:

        result = repl.run(code)

    except BaseException as e:

        return f"Failed to execute. Error: {repr(e)}"

    result_str = f"Successfully executed:\n```python\n{code}\n```\nStdout: {result}"

    return (

        result_str + "\n\nIf you have completed all tasks, respond with FINAL ANSWER."

    )


## Create graph



Now that we've defined our tools and made some helper functions, will create the individual agents below and tell them how to talk to each other using LangGraph.


### Define Agent Nodes



We now need to define the nodes.



First, we'll create a utility to create a system prompt for each agent.


def make_system_prompt(suffix: str) -> str:

    return (

        "You are a helpful AI assistant, collaborating with other assistants."

        " Use the provided tools to progress towards answering the question."

        " If you are unable to fully answer, that's OK, another assistant with different tools "

        " will help where you left off. Execute what you can to make progress."

        " If you or any of the other assistants have the final answer or deliverable,"

        " prefix your response with FINAL ANSWER so the team knows to stop."

        f"\n{suffix}"

    )


from typing import Literal



from langchain_core.messages import BaseMessage, HumanMessage

from langchain_anthropic import ChatAnthropic

from langgraph.prebuilt import create_react_agent

from langgraph.graph import MessagesState, END

from langgraph.types import Command





llm = ChatAnthropic(model="claude-3-5-sonnet-latest")





def get_next_node(last_message: BaseMessage, goto: str):

    if "FINAL ANSWER" in last_message.content:

        # Any agent decided the work is done

        return END

    return goto





# Research agent and node

research_agent = create_react_agent(

    llm,

    tools=[tavily_tool],

    prompt=make_system_prompt(

        "You can only do research. You are working with a chart generator colleague."

    ),

)





def research_node(

    state: MessagesState,

) -> Command[Literal["chart_generator", END]]:

    result = research_agent.invoke(state)

    goto = get_next_node(result["messages"][-1], "chart_generator")

    # wrap in a human message, as not all providers allow

    # AI message at the last position of the input messages list

    result["messages"][-1] = HumanMessage(

        content=result["messages"][-1].content, name="researcher"

    )

    return Command(

        update={

            # share internal message history of research agent with other agents

            "messages": result["messages"],

        },

        goto=goto,

    )





# Chart generator agent and node

# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED

chart_agent = create_react_agent(

    llm,

    [python_repl_tool],

    prompt=make_system_prompt(

        "You can only generate charts. You are working with a researcher colleague."

    ),

)





def chart_node(state: MessagesState) -> Command[Literal["researcher", END]]:

    result = chart_agent.invoke(state)

    goto = get_next_node(result["messages"][-1], "researcher")

    # wrap in a human message, as not all providers allow

    # AI message at the last position of the input messages list

    result["messages"][-1] = HumanMessage(

        content=result["messages"][-1].content, name="chart_generator"

    )

    return Command(

        update={

            # share internal message history of chart agent with other agents

            "messages": result["messages"],

        },

        goto=goto,

    )


### Define the Graph



We can now put it all together and define the graph!


from langgraph.graph import StateGraph, START



workflow = StateGraph(MessagesState)

workflow.add_node("researcher", research_node)

workflow.add_node("chart_generator", chart_node)



workflow.add_edge(START, "researcher")

graph = workflow.compile()


from IPython.display import Image, display



try:

    display(Image(graph.get_graph().draw_mermaid_png()))

except Exception:

    # This requires some extra dependencies and is optional

    pass


## Invoke



With the graph created, you can invoke it! Let's have it chart some stats for us.


events = graph.stream(

    {

        "messages": [

            (

                "user",

                "First, get the UK's GDP over the past 5 years, then make a line chart of it. "

                "Once you make the chart, finish.",

            )

        ],

    },

    # Maximum number of steps to take in the graph

    {"recursion_limit": 150},

)

for s in events:

    print(s)

    print("----")







##### FILE: plan-and-execute.ipynb #####


# Plan-and-Execute



This notebook shows how to create a "plan-and-execute" style agent. This is heavily inspired by the [Plan-and-Solve](https://arxiv.org/abs/2305.04091) paper as well as the [Baby-AGI](https://github.com/yoheinakajima/babyagi) project.



The core idea is to first come up with a multi-step plan, and then go through that plan one item at a time.

After accomplishing a particular task, you can then revisit the plan and modify as appropriate.





The general computational graph looks like the following:



![plan-and-execute diagram](attachment:86cf6404-3d9b-41cb-ab97-5e451f576620.png)





This compares to a typical [ReAct](https://arxiv.org/abs/2210.03629) style agent where you think one step at a time.

The advantages of this "plan-and-execute" style agent are:



1. Explicit long term planning (which even really strong LLMs can struggle with)

2. Ability to use smaller/weaker models for the execution step, only using larger/better models for the planning step





The following walkthrough demonstrates how to do so in LangGraph. The resulting agent will leave a trace like the following example: ([link](https://smith.langchain.com/public/d46e24d3-dda6-44d5-9550-b618fca4e0d4/r)).


## Setup



First, we need to install the packages required.


%%capture --no-stderr

%pip install --quiet -U langgraph langchain-community langchain-openai tavily-python


Next, we need to set API keys for OpenAI (the LLM we will use) and Tavily (the search tool we will use)


import getpass

import os





def _set_env(var: str):

    if not os.environ.get(var):

        os.environ[var] = getpass.getpass(f"{var}: ")





_set_env("OPENAI_API_KEY")

_set_env("TAVILY_API_KEY")


<div class="admonition tip">

    <p class="admonition-title">Set up <a href="https://smith.langchain.com">LangSmith</a> for LangGraph development</p>

    <p style="padding-top: 5px;">

        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph  read more about how to get started <a href="https://docs.smith.langchain.com">here</a>. 

    </p>

</div>


## Define Tools



We will first define the tools we want to use. For this simple example, we will use a built-in search tool via Tavily. However, it is really easy to create your own tools - see documentation [here](https://python.langchain.com/docs/how_to/custom_tools) on how to do that.


from langchain_community.tools.tavily_search import TavilySearchResults



tools = [TavilySearchResults(max_results=3)]


## Define our Execution Agent



Now we will create the execution agent we want to use to execute tasks. 

Note that for this example, we will be using the same execution agent for each task, but this doesn't HAVE to be the case.


from langchain import hub

from langchain_openai import ChatOpenAI



from langgraph.prebuilt import create_react_agent



# Choose the LLM that will drive the agent

llm = ChatOpenAI(model="gpt-4-turbo-preview")

prompt = "You are a helpful assistant."

agent_executor = create_react_agent(llm, tools, prompt=prompt)


agent_executor.invoke({"messages": [("user", "who is the winnner of the us open")]})


## Define the State



Let's now start by defining the state the track for this agent.



First, we will need to track the current plan. Let's represent that as a list of strings.



Next, we should track previously executed steps. Let's represent that as a list of tuples (these tuples will contain the step and then the result)



Finally, we need to have some state to represent the final response as well as the original input.


import operator

from typing import Annotated, List, Tuple

from typing_extensions import TypedDict





class PlanExecute(TypedDict):

    input: str

    plan: List[str]

    past_steps: Annotated[List[Tuple], operator.add]

    response: str


## Planning Step



Let's now think about creating the planning step. This will use function calling to create a plan.


<div class="admonition note">

    <p class="admonition-title">Using Pydantic with LangChain</p>

    <p>

        This notebook uses Pydantic v2 <code>BaseModel</code>, which requires <code>langchain-core >= 0.3</code>. Using <code>langchain-core < 0.3</code> will result in errors due to mixing of Pydantic v1 and v2 <code>BaseModels</code>.

    </p>

</div>


from pydantic import BaseModel, Field





class Plan(BaseModel):

    """Plan to follow in future"""



    steps: List[str] = Field(

        description="different steps to follow, should be in sorted order"

    )


from langchain_core.prompts import ChatPromptTemplate



planner_prompt = ChatPromptTemplate.from_messages(

    [

        (

            "system",

            """For the given objective, come up with a simple step by step plan. \

This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \

The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.""",

        ),

        ("placeholder", "{messages}"),

    ]

)

planner = planner_prompt | ChatOpenAI(

    model="gpt-4o", temperature=0

).with_structured_output(Plan)


planner.invoke(

    {

        "messages": [

            ("user", "what is the hometown of the current Australia open winner?")

        ]

    }

)


## Re-Plan Step



Now, let's create a step that re-does the plan based on the result of the previous step.


from typing import Union





class Response(BaseModel):

    """Response to user."""



    response: str





class Act(BaseModel):

    """Action to perform."""



    action: Union[Response, Plan] = Field(

        description="Action to perform. If you want to respond to user, use Response. "

        "If you need to further use tools to get the answer, use Plan."

    )





replanner_prompt = ChatPromptTemplate.from_template(

    """For the given objective, come up with a simple step by step plan. \

This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \

The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.



Your objective was this:

{input}



Your original plan was this:

{plan}



You have currently done the follow steps:

{past_steps}



Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan."""

)





replanner = replanner_prompt | ChatOpenAI(

    model="gpt-4o", temperature=0

).with_structured_output(Act)


## Create the Graph



We can now create the graph!


from typing import Literal

from langgraph.graph import END





async def execute_step(state: PlanExecute):

    plan = state["plan"]

    plan_str = "\n".join(f"{i+1}. {step}" for i, step in enumerate(plan))

    task = plan[0]

    task_formatted = f"""For the following plan:

{plan_str}\n\nYou are tasked with executing step {1}, {task}."""

    agent_response = await agent_executor.ainvoke(

        {"messages": [("user", task_formatted)]}

    )

    return {

        "past_steps": [(task, agent_response["messages"][-1].content)],

    }





async def plan_step(state: PlanExecute):

    plan = await planner.ainvoke({"messages": [("user", state["input"])]})

    return {"plan": plan.steps}





async def replan_step(state: PlanExecute):

    output = await replanner.ainvoke(state)

    if isinstance(output.action, Response):

        return {"response": output.action.response}

    else:

        return {"plan": output.action.steps}





def should_end(state: PlanExecute):

    if "response" in state and state["response"]:

        return END

    else:

        return "agent"


from langgraph.graph import StateGraph, START



workflow = StateGraph(PlanExecute)



# Add the plan node

workflow.add_node("planner", plan_step)



# Add the execution step

workflow.add_node("agent", execute_step)



# Add a replan node

workflow.add_node("replan", replan_step)



workflow.add_edge(START, "planner")



# From plan we go to agent

workflow.add_edge("planner", "agent")



# From agent, we replan

workflow.add_edge("agent", "replan")



workflow.add_conditional_edges(

    "replan",

    # Next, we pass in the function that will determine which node is called next.

    should_end,

    ["agent", END],

)



# Finally, we compile it!

# This compiles it into a LangChain Runnable,

# meaning you can use it as you would any other runnable

app = workflow.compile()


from IPython.display import Image, display



display(Image(app.get_graph(xray=True).draw_mermaid_png()))


config = {"recursion_limit": 50}

inputs = {"input": "what is the hometown of the mens 2024 Australia open winner?"}

async for event in app.astream(inputs, config=config):

    for k, v in event.items():

        if k != "__end__":

            print(v)


## Conclusion



Congrats on making a plan-and-execute agent! One known limitations of the above design is that each task is still executed in sequence, meaning embarrassingly parallel operations all add to the total execution time. You could improve on this by having each task represented as a DAG (similar to LLMCompiler), rather than a regular list.







##### FILE: reflection.ipynb #####


# Reflection





In the context of LLM agent building, reflection refers to the process of prompting an LLM to observe its past steps (along with potential observations from tools/the environment) to assess the quality of the chosen actions.

This is then used downstream for things like re-planning, search, or evaluation.



![Reflection](attachment:fc393f72-3401-4b86-b0d3-e4789b640a27.png)



This notebook demonstrates a very simple form of reflection in LangGraph.


## Setup



First, let's install our required packages and set our API keys


%pip install -U --quiet  langgraph langchain-fireworks

%pip install -U --quiet tavily-python


import getpass

import os





def _set_if_undefined(var: str) -> None:

    if os.environ.get(var):

        return

    os.environ[var] = getpass.getpass(var)





_set_if_undefined("TAVILY_API_KEY")

_set_if_undefined("FIREWORKS_API_KEY")


<div class="admonition tip">

    <p class="admonition-title">Set up <a href="https://smith.langchain.com">LangSmith</a> for LangGraph development</p>

    <p style="padding-top: 5px;">

        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph  read more about how to get started <a href="https://docs.smith.langchain.com">here</a>. 

    </p>

</div>


## Generate



For our example, we will create a "5 paragraph essay" generator. First, create the generator:



from langchain_core.messages import AIMessage, BaseMessage, HumanMessage

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

from langchain_fireworks import ChatFireworks



prompt = ChatPromptTemplate.from_messages(

    [

        (

            "system",

            "You are an essay assistant tasked with writing excellent 5-paragraph essays."

            " Generate the best essay possible for the user's request."

            " If the user provides critique, respond with a revised version of your previous attempts.",

        ),

        MessagesPlaceholder(variable_name="messages"),

    ]

)

llm = ChatFireworks(

    model="accounts/fireworks/models/mixtral-8x7b-instruct", max_tokens=32768

)

generate = prompt | llm


essay = ""

request = HumanMessage(

    content="Write an essay on why the little prince is relevant in modern childhood"

)

for chunk in generate.stream({"messages": [request]}):

    print(chunk.content, end="")

    essay += chunk.content


### Reflect


reflection_prompt = ChatPromptTemplate.from_messages(

    [

        (

            "system",

            "You are a teacher grading an essay submission. Generate critique and recommendations for the user's submission."

            " Provide detailed recommendations, including requests for length, depth, style, etc.",

        ),

        MessagesPlaceholder(variable_name="messages"),

    ]

)

reflect = reflection_prompt | llm


reflection = ""

for chunk in reflect.stream({"messages": [request, HumanMessage(content=essay)]}):

    print(chunk.content, end="")

    reflection += chunk.content


### Repeat



And... that's all there is too it! You can repeat in a loop for a fixed number of steps, or use an LLM (or other check) to decide when the finished product is good enough.


for chunk in generate.stream(

    {"messages": [request, AIMessage(content=essay), HumanMessage(content=reflection)]}

):

    print(chunk.content, end="")


## Define graph



Now that we've shown each step in isolation, we can wire it up in a graph.


from typing import Annotated, List, Sequence

from langgraph.graph import END, StateGraph, START

from langgraph.graph.message import add_messages

from langgraph.checkpoint.memory import MemorySaver

from typing_extensions import TypedDict





class State(TypedDict):

    messages: Annotated[list, add_messages]





async def generation_node(state: State) -> State:

    return {"messages": [await generate.ainvoke(state["messages"])]}





async def reflection_node(state: State) -> State:

    # Other messages we need to adjust

    cls_map = {"ai": HumanMessage, "human": AIMessage}

    # First message is the original user request. We hold it the same for all nodes

    translated = [state["messages"][0]] + [

        cls_map[msg.type](content=msg.content) for msg in state["messages"][1:]

    ]

    res = await reflect.ainvoke(translated)

    # We treat the output of this as human feedback for the generator

    return {"messages": [HumanMessage(content=res.content)]}





builder = StateGraph(State)

builder.add_node("generate", generation_node)

builder.add_node("reflect", reflection_node)

builder.add_edge(START, "generate")





def should_continue(state: State):

    if len(state["messages"]) > 6:

        # End after 3 iterations

        return END

    return "reflect"





builder.add_conditional_edges("generate", should_continue)

builder.add_edge("reflect", "generate")

memory = MemorySaver()

graph = builder.compile(checkpointer=memory)


config = {"configurable": {"thread_id": "1"}}


async for event in graph.astream(

    {

        "messages": [

            HumanMessage(

                content="Generate an essay on the topicality of The Little Prince and its message in modern life"

            )

        ],

    },

    config,

):

    print(event)

    print("---")


state = graph.get_state(config)


ChatPromptTemplate.from_messages(state.values["messages"]).pretty_print()


## Conclusion



Now that you've applied reflection to an LLM agent, I'll note one thing: self-reflection is inherently cyclic: it is much more effective if the reflection step has additional context or feedback (from tool observations, checks, etc.). If, like in the scenario above, the reflection step simply prompts the LLM to reflect on its output, it can still benefit the output quality (since the LLM then has multiple "shots" at getting a good output), but it's less guaranteed.








##### FILE: reflexion.ipynb #####


# Reflexion



[Reflexion](https://arxiv.org/abs/2303.11366) by Shinn, et. al., is an architecture designed to learn through verbal feedback and self-reflection. The agent explicitly critiques its responses for tasks to generate a higher quality final response, at the expense of longer execution time.



![reflexion diagram](attachment:2f424259-8d89-4f4e-94c4-d668a36d8ca2.png)



The paper outlines 3 main components:



1. Actor (agent) with self-reflection

2. External evaluator (task-specific, e.g. code compilation steps)

3. Episodic memory that stores the reflections from (1).



In their code, the last two components are very task-specific, so in this notebook, you will build the _actor_ in LangGraph.



To skip to the graph definition, see the [Construct Graph section](#Construct-Graph) below.


## Setup



Install `langgraph` (for the framework), `langchain_openai` (for the LLM), and `langchain` + `tavily-python` (for the search engine).



We will use tavily search as a tool. You can get an API key [here](https://app.tavily.com/sign-in) or replace with a different tool of your choosing.


%pip install -U --quiet langgraph langchain_anthropic tavily-python


import getpass

import os





def _set_if_undefined(var: str) -> None:

    if os.environ.get(var):

        return

    os.environ[var] = getpass.getpass(var)





_set_if_undefined("ANTHROPIC_API_KEY")

_set_if_undefined("TAVILY_API_KEY")


<div class="admonition tip">

    <p class="admonition-title">Set up <a href="https://smith.langchain.com">LangSmith</a> for LangGraph development</p>

    <p style="padding-top: 5px;">

        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph  read more about how to get started <a href="https://docs.smith.langchain.com">here</a>. 

    </p>

</div>   



### Define our LLM


from langchain_anthropic import ChatAnthropic



llm = ChatAnthropic(model="claude-3-5-sonnet-20240620")

# You could also use OpenAI or another provider

# from langchain_openai import ChatOpenAI



# llm = ChatOpenAI(model="gpt-4-turbo-preview")


## Actor (with reflection)



The main component of Reflexion is the "actor", which is an agent that reflects on its response and re-executes to improve based on self-critique. It's main sub-components include:

1. Tools/tool execution

2. Initial responder: generate an initial response (and self-reflection)

3. Revisor: re-respond (and reflec) based on previous reflections



We'll first define the tool execution context.



#### Construct tools


from langchain_community.tools.tavily_search import TavilySearchResults

from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper



search = TavilySearchAPIWrapper()

tavily_tool = TavilySearchResults(api_wrapper=search, max_results=5)


#### Initial responder


<div class="admonition note">

    <p class="admonition-title">Using Pydantic with LangChain</p>

    <p>

        This notebook uses Pydantic v2 <code>BaseModel</code>, which requires <code>langchain-core >= 0.3</code>. Using <code>langchain-core < 0.3</code> will result in errors due to mixing of Pydantic v1 and v2 <code>BaseModels</code>.

    </p>

</div>


from langchain_core.messages import HumanMessage, ToolMessage

from langchain_core.output_parsers.openai_tools import PydanticToolsParser

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

from pydantic import ValidationError



from pydantic import BaseModel, Field





class Reflection(BaseModel):

    missing: str = Field(description="Critique of what is missing.")

    superfluous: str = Field(description="Critique of what is superfluous")





class AnswerQuestion(BaseModel):

    """Answer the question. Provide an answer, reflection, and then follow up with search queries to improve the answer."""



    answer: str = Field(description="~250 word detailed answer to the question.")

    reflection: Reflection = Field(description="Your reflection on the initial answer.")

    search_queries: list[str] = Field(

        description="1-3 search queries for researching improvements to address the critique of your current answer."

    )





class ResponderWithRetries:

    def __init__(self, runnable, validator):

        self.runnable = runnable

        self.validator = validator



    def respond(self, state: dict):

        response = []

        for attempt in range(3):

            response = self.runnable.invoke(

                {"messages": state["messages"]}, {"tags": [f"attempt:{attempt}"]}

            )

            try:

                self.validator.invoke(response)

                return {"messages": response}

            except ValidationError as e:

                state = state + [

                    response,

                    ToolMessage(

                        content=f"{repr(e)}\n\nPay close attention to the function schema.\n\n"

                        + self.validator.schema_json()

                        + " Respond by fixing all validation errors.",

                        tool_call_id=response.tool_calls[0]["id"],

                    ),

                ]

        return {"messages": response}


import datetime



actor_prompt_template = ChatPromptTemplate.from_messages(

    [

        (

            "system",

            """You are expert researcher.

Current time: {time}



1. {first_instruction}

2. Reflect and critique your answer. Be severe to maximize improvement.

3. Recommend search queries to research information and improve your answer.""",

        ),

        MessagesPlaceholder(variable_name="messages"),

        (

            "user",

            "\n\n<system>Reflect on the user's original question and the"

            " actions taken thus far. Respond using the {function_name} function.</reminder>",

        ),

    ]

).partial(

    time=lambda: datetime.datetime.now().isoformat(),

)

initial_answer_chain = actor_prompt_template.partial(

    first_instruction="Provide a detailed ~250 word answer.",

    function_name=AnswerQuestion.__name__,

) | llm.bind_tools(tools=[AnswerQuestion])

validator = PydanticToolsParser(tools=[AnswerQuestion])



first_responder = ResponderWithRetries(

    runnable=initial_answer_chain, validator=validator

)


example_question = "Why is reflection useful in AI?"

initial = first_responder.respond(

    {"messages": [HumanMessage(content=example_question)]}

)


#### Revision



The second part of the actor is a revision step.


revise_instructions = """Revise your previous answer using the new information.

    - You should use the previous critique to add important information to your answer.

        - You MUST include numerical citations in your revised answer to ensure it can be verified.

        - Add a "References" section to the bottom of your answer (which does not count towards the word limit). In form of:

            - [1] https://example.com

            - [2] https://example.com

    - You should use the previous critique to remove superfluous information from your answer and make SURE it is not more than 250 words.

"""





# Extend the initial answer schema to include references.

# Forcing citation in the model encourages grounded responses

class ReviseAnswer(AnswerQuestion):

    """Revise your original answer to your question. Provide an answer, reflection,



    cite your reflection with references, and finally

    add search queries to improve the answer."""



    references: list[str] = Field(

        description="Citations motivating your updated answer."

    )





revision_chain = actor_prompt_template.partial(

    first_instruction=revise_instructions,

    function_name=ReviseAnswer.__name__,

) | llm.bind_tools(tools=[ReviseAnswer])

revision_validator = PydanticToolsParser(tools=[ReviseAnswer])



revisor = ResponderWithRetries(runnable=revision_chain, validator=revision_validator)


import json



revised = revisor.respond(

    {

        "messages": [

            HumanMessage(content=example_question),

            initial["messages"],

            ToolMessage(

                tool_call_id=initial["messages"].tool_calls[0]["id"],

                content=json.dumps(

                    tavily_tool.invoke(

                        {

                            "query": initial["messages"].tool_calls[0]["args"][

                                "search_queries"

                            ][0]

                        }

                    )

                ),

            ),

        ]

    }

)

revised["messages"]


## Create Tool Node



Next, create a node to execute the tool calls. While we give the LLMs different schema names (and use those for validation), we want them both to route to the same tool.


from langchain_core.tools import StructuredTool



from langgraph.prebuilt import ToolNode





def run_queries(search_queries: list[str], **kwargs):

    """Run the generated queries."""

    return tavily_tool.batch([{"query": query} for query in search_queries])





tool_node = ToolNode(

    [

        StructuredTool.from_function(run_queries, name=AnswerQuestion.__name__),

        StructuredTool.from_function(run_queries, name=ReviseAnswer.__name__),

    ]

)


## Construct Graph





Now we can wire all our components together.


from typing import Literal



from langgraph.graph import END, StateGraph, START

from langgraph.graph.message import add_messages

from typing import Annotated

from typing_extensions import TypedDict





class State(TypedDict):

    messages: Annotated[list, add_messages]





MAX_ITERATIONS = 5

builder = StateGraph(State)

builder.add_node("draft", first_responder.respond)





builder.add_node("execute_tools", tool_node)

builder.add_node("revise", revisor.respond)

# draft -> execute_tools

builder.add_edge("draft", "execute_tools")

# execute_tools -> revise

builder.add_edge("execute_tools", "revise")



# Define looping logic:





def _get_num_iterations(state: list):

    i = 0

    for m in state[::-1]:

        if m.type not in {"tool", "ai"}:

            break

        i += 1

    return i





def event_loop(state: list):

    # in our case, we'll just stop after N plans

    num_iterations = _get_num_iterations(state["messages"])

    if num_iterations > MAX_ITERATIONS:

        return END

    return "execute_tools"





# revise -> execute_tools OR end

builder.add_conditional_edges("revise", event_loop, ["execute_tools", END])

builder.add_edge(START, "draft")

graph = builder.compile()


from IPython.display import Image, display



try:

    display(Image(graph.get_graph().draw_mermaid_png()))

except Exception:

    # This requires some extra dependencies and is optional

    pass


events = graph.stream(

    {"messages": [("user", "How should we handle the climate crisis?")]},

    stream_mode="values",

)

for i, step in enumerate(events):

    print(f"Step {i}")

    step["messages"][-1].pretty_print()


## Conclusion



Congrats on building a Reflexion actor! I'll leave you with a few observations to save you some time when choosing which parts of this agent to adapt to your workflow:

1. This agent trades off execution time for quality. It explicitly forces the agent to critique and revise the output over several steps, which usually (not always) increases the response quality but takes much longer to return a final answer

2. The 'reflections' can be paired with additional external feedback (such as validators), to further guide the actor.

3. In the paper, 1 environment (AlfWorld) uses external memory. It does this by storing summaries of the reflections to an external store and using them in subsequent trials/invocations.







##### FILE: storm.ipynb #####


# Web Research (STORM)



[STORM](https://arxiv.org/abs/2402.14207) is a research assistant designed by Shao, et. al that extends the idea of "outline-driven RAG" for richer article generation.



STORM is designed to generate Wikipedia-style ariticles on a user-provided topic. It applies two main insights to produce more organized and comprehensive articles:



1. Creating an outline (planning) by querying similar topics helps improve coverage.

2. Multi-perspective, grounded (in search) conversation simulation helps increase the reference count and information density. 



The control flow looks like the diagram below.



![storm.png](attachment:bdc25ea2-123b-46b1-b9f5-fdd345ecbc73.png)



STORM has a few main stages:



1. Generate initial outline + Survey related subjects

2. Identify distinct perspectives

3. "Interview subject matter experts" (role-playing LLMs)

4. Refine outline (using references)

5. Write sections, then write article





The expert interviews stage occurs between the role-playing article writer and a research expert. The "expert" is able to query external knowledge and respond to pointed questions, saving cited sources to a vectorstore so that the later refinement stages can synthesize the full article.



There are a couple hyperparameters you can set to restrict the (potentially) infinite research breadth:



N: Number of perspectives to survey / use (Steps 2->3)

M: Max number of conversation turns in step (Step 3)





## Setup



First, let's install the required packages and set our API keys


%%capture --no-stderr

%pip install -U langchain_community langchain_openai langchain_fireworks langgraph wikipedia duckduckgo-search tavily-python


# Uncomment if you want to draw the pretty graph diagrams.

# If you are on MacOS, you will need to run brew install graphviz before installing and update some environment flags

# ! brew install graphviz

# !CFLAGS="-I $(brew --prefix graphviz)/include" LDFLAGS="-L $(brew --prefix graphviz)/lib" pip install -U pygraphviz


import getpass

import os





def _set_env(var: str):

    if os.environ.get(var):

        return

    os.environ[var] = getpass.getpass(var + ":")





_set_env("OPENAI_API_KEY")

_set_env("TAVILY_API_KEY")


<div class="admonition tip">

    <p class="admonition-title">Set up <a href="https://smith.langchain.com">LangSmith</a> for LangGraph development</p>

    <p style="padding-top: 5px;">

        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph  read more about how to get started <a href="https://docs.smith.langchain.com">here</a>. 

    </p>

</div>   


### Select LLMs



We will have a faster LLM do most of the work, but a slower, long-context model to distill the conversations and write the final report.


from langchain_openai import ChatOpenAI



fast_llm = ChatOpenAI(model="gpt-4o-mini")

# Uncomment for a Fireworks model

# fast_llm = ChatFireworks(model="accounts/fireworks/models/firefunction-v1", max_tokens=32_000)

long_context_llm = ChatOpenAI(model="gpt-4o")


## Generate Initial Outline



For many topics, your LLM may have an initial idea of the important and related topics. We can generate an initial

outline to be refined after our research. Below, we will use our "fast" llm to generate the outline.


<div class="admonition note">

    <p class="admonition-title">Using Pydantic with LangChain</p>

    <p>

        This notebook uses Pydantic v2 <code>BaseModel</code>, which requires <code>langchain-core >= 0.3</code>. Using <code>langchain-core < 0.3</code> will result in errors due to mixing of Pydantic v1 and v2 <code>BaseModels</code>.

    </p>

</div>


from typing import List, Optional



from langchain_core.prompts import ChatPromptTemplate



from pydantic import BaseModel, Field, field_validator



direct_gen_outline_prompt = ChatPromptTemplate.from_messages(

    [

        (

            "system",

            "You are a Wikipedia writer. Write an outline for a Wikipedia page about a user-provided topic. Be comprehensive and specific.",

        ),

        ("user", "{topic}"),

    ]

)





class Subsection(BaseModel):

    subsection_title: str = Field(..., title="Title of the subsection")

    description: str = Field(..., title="Content of the subsection")



    @property

    def as_str(self) -> str:

        return f"### {self.subsection_title}\n\n{self.description}".strip()





class Section(BaseModel):

    section_title: str = Field(..., title="Title of the section")

    description: str = Field(..., title="Content of the section")

    subsections: Optional[List[Subsection]] = Field(

        default=None,

        title="Titles and descriptions for each subsection of the Wikipedia page.",

    )



    @property

    def as_str(self) -> str:

        subsections = "\n\n".join(

            f"### {subsection.subsection_title}\n\n{subsection.description}"

            for subsection in self.subsections or []

        )

        return f"## {self.section_title}\n\n{self.description}\n\n{subsections}".strip()





class Outline(BaseModel):

    page_title: str = Field(..., title="Title of the Wikipedia page")

    sections: List[Section] = Field(

        default_factory=list,

        title="Titles and descriptions for each section of the Wikipedia page.",

    )



    @property

    def as_str(self) -> str:

        sections = "\n\n".join(section.as_str for section in self.sections)

        return f"# {self.page_title}\n\n{sections}".strip()





generate_outline_direct = direct_gen_outline_prompt | fast_llm.with_structured_output(

    Outline

)


example_topic = "Impact of million-plus token context window language models on RAG"



initial_outline = generate_outline_direct.invoke({"topic": example_topic})



print(initial_outline.as_str)


## Expand Topics



While language models do store some Wikipedia-like knowledge in their parameters, you will get better results by incorporating relevant and recent information using a search engine.



We will start our search by generating a list of related topics, sourced from Wikipedia.


gen_related_topics_prompt = ChatPromptTemplate.from_template(

    """I'm writing a Wikipedia page for a topic mentioned below. Please identify and recommend some Wikipedia pages on closely related subjects. I'm looking for examples that provide insights into interesting aspects commonly associated with this topic, or examples that help me understand the typical content and structure included in Wikipedia pages for similar topics.



Please list the as many subjects and urls as you can.



Topic of interest: {topic}

"""

)





class RelatedSubjects(BaseModel):

    topics: List[str] = Field(

        description="Comprehensive list of related subjects as background research.",

    )





expand_chain = gen_related_topics_prompt | fast_llm.with_structured_output(

    RelatedSubjects

)


related_subjects = await expand_chain.ainvoke({"topic": example_topic})

related_subjects


## Generate Perspectives



From these related subjects, we can select representative Wikipedia editors as "subject matter experts" with distinct

backgrounds and affiliations. These will help distribute the search process to encourage a more well-rounded final report.


class Editor(BaseModel):

    affiliation: str = Field(

        description="Primary affiliation of the editor.",

    )

    name: str = Field(

        description="Name of the editor.", pattern=r"^[a-zA-Z0-9_-]{1,64}$"

    )

    role: str = Field(

        description="Role of the editor in the context of the topic.",

    )

    description: str = Field(

        description="Description of the editor's focus, concerns, and motives.",

    )



    @field_validator("name", mode="before")

    def sanitize_name(cls, value: str) -> str:

        return value.replace(" ", "").replace(".", "")



    @property

    def persona(self) -> str:

        return f"Name: {self.name}\nRole: {self.role}\nAffiliation: {self.affiliation}\nDescription: {self.description}\n"





class Perspectives(BaseModel):

    editors: List[Editor] = Field(

        description="Comprehensive list of editors with their roles and affiliations.",

        # Add a pydantic validation/restriction to be at most M editors

    )





gen_perspectives_prompt = ChatPromptTemplate.from_messages(

    [

        (

            "system",

            """You need to select a diverse (and distinct) group of Wikipedia editors who will work together to create a comprehensive article on the topic. Each of them represents a different perspective, role, or affiliation related to this topic.\

    You can use other Wikipedia pages of related topics for inspiration. For each editor, add a description of what they will focus on.



    Wiki page outlines of related topics for inspiration:

    {examples}""",

        ),

        ("user", "Topic of interest: {topic}"),

    ]

)



gen_perspectives_chain = gen_perspectives_prompt | fast_llm.with_structured_output(

    Perspectives, method="function_calling"

)


from langchain_community.retrievers import WikipediaRetriever

from langchain_core.runnables import RunnableLambda

from langchain_core.runnables import chain as as_runnable



wikipedia_retriever = WikipediaRetriever(load_all_available_meta=True, top_k_results=1)





def format_doc(doc, max_length=1000):

    related = "- ".join(doc.metadata["categories"])

    return f"### {doc.metadata['title']}\n\nSummary: {doc.page_content}\n\nRelated\n{related}"[

        :max_length

    ]





def format_docs(docs):

    return "\n\n".join(format_doc(doc) for doc in docs)





@as_runnable

async def survey_subjects(topic: str):

    related_subjects = await expand_chain.ainvoke({"topic": topic})

    retrieved_docs = await wikipedia_retriever.abatch(

        related_subjects.topics, return_exceptions=True

    )

    all_docs = []

    for docs in retrieved_docs:

        if isinstance(docs, BaseException):

            continue

        all_docs.extend(docs)

    formatted = format_docs(all_docs)

    return await gen_perspectives_chain.ainvoke({"examples": formatted, "topic": topic})


perspectives = await survey_subjects.ainvoke(example_topic)


perspectives.model_dump()


## Expert Dialog



Now the true fun begins, each wikipedia writer is primed to role-play using the perspectives presented above. It will ask a series of questions of a second "domain expert" with access to a search engine. This generate content to generate a refined outline as well as an updated index of reference documents.





### Interview State



The conversation is cyclic, so we will construct it within its own graph. The State will contain messages, the reference docs, and the editor (with its own "persona") to make it easy to parallelize these conversations.


from typing import Annotated



from langchain_core.messages import AnyMessage

from typing_extensions import TypedDict



from langgraph.graph import END, StateGraph, START





def add_messages(left, right):

    if not isinstance(left, list):

        left = [left]

    if not isinstance(right, list):

        right = [right]

    return left + right





def update_references(references, new_references):

    if not references:

        references = {}

    references.update(new_references)

    return references





def update_editor(editor, new_editor):

    # Can only set at the outset

    if not editor:

        return new_editor

    return editor





class InterviewState(TypedDict):

    messages: Annotated[List[AnyMessage], add_messages]

    references: Annotated[Optional[dict], update_references]

    editor: Annotated[Optional[Editor], update_editor]


#### Dialog Roles



The graph will have two participants: the wikipedia editor (`generate_question`), who asks questions based on its assigned role, and a domain expert (`gen_answer_chain), who uses a search engine to answer the questions as accurately as possible.


from langchain_core.messages import AIMessage, HumanMessage, ToolMessage

from langchain_core.prompts import MessagesPlaceholder



gen_qn_prompt = ChatPromptTemplate.from_messages(

    [

        (

            "system",

            """You are an experienced Wikipedia writer and want to edit a specific page. \

Besides your identity as a Wikipedia writer, you have a specific focus when researching the topic. \

Now, you are chatting with an expert to get information. Ask good questions to get more useful information.



When you have no more questions to ask, say "Thank you so much for your help!" to end the conversation.\

Please only ask one question at a time and don't ask what you have asked before.\

Your questions should be related to the topic you want to write.

Be comprehensive and curious, gaining as much unique insight from the expert as possible.\



Stay true to your specific perspective:



{persona}""",

        ),

        MessagesPlaceholder(variable_name="messages", optional=True),

    ]

)





def tag_with_name(ai_message: AIMessage, name: str):

    ai_message.name = name

    return ai_message





def swap_roles(state: InterviewState, name: str):

    converted = []

    for message in state["messages"]:

        if isinstance(message, AIMessage) and message.name != name:

            message = HumanMessage(**message.model_dump(exclude={"type"}))

        converted.append(message)

    return {"messages": converted}





@as_runnable

async def generate_question(state: InterviewState):

    editor = state["editor"]

    gn_chain = (

        RunnableLambda(swap_roles).bind(name=editor.name)

        | gen_qn_prompt.partial(persona=editor.persona)

        | fast_llm

        | RunnableLambda(tag_with_name).bind(name=editor.name)

    )

    result = await gn_chain.ainvoke(state)

    return {"messages": [result]}


messages = [

    HumanMessage(f"So you said you were writing an article on {example_topic}?")

]

question = await generate_question.ainvoke(

    {

        "editor": perspectives.editors[0],

        "messages": messages,

    }

)



question["messages"][0].content


#### Answer questions



The `gen_answer_chain` first generates queries (query expansion) to answer the editor's question, then responds with citations.


class Queries(BaseModel):

    queries: List[str] = Field(

        description="Comprehensive list of search engine queries to answer the user's questions.",

    )





gen_queries_prompt = ChatPromptTemplate.from_messages(

    [

        (

            "system",

            "You are a helpful research assistant. Query the search engine to answer the user's questions.",

        ),

        MessagesPlaceholder(variable_name="messages", optional=True),

    ]

)

gen_queries_chain = gen_queries_prompt | fast_llm.with_structured_output(

    Queries, include_raw=True, method="function_calling"

)


queries = await gen_queries_chain.ainvoke(

    {"messages": [HumanMessage(content=question["messages"][0].content)]}

)

queries["parsed"].queries


class AnswerWithCitations(BaseModel):

    answer: str = Field(

        description="Comprehensive answer to the user's question with citations.",

    )

    cited_urls: List[str] = Field(

        description="List of urls cited in the answer.",

    )



    @property

    def as_str(self) -> str:

        return f"{self.answer}\n\nCitations:\n\n" + "\n".join(

            f"[{i+1}]: {url}" for i, url in enumerate(self.cited_urls)

        )





gen_answer_prompt = ChatPromptTemplate.from_messages(

    [

        (

            "system",

            """You are an expert who can use information effectively. You are chatting with a Wikipedia writer who wants\

 to write a Wikipedia page on the topic you know. You have gathered the related information and will now use the information to form a response.



Make your response as informative as possible and make sure every sentence is supported by the gathered information.

Each response must be backed up by a citation from a reliable source, formatted as a footnote, reproducing the URLS after your response.""",

        ),

        MessagesPlaceholder(variable_name="messages", optional=True),

    ]

)



gen_answer_chain = gen_answer_prompt | fast_llm.with_structured_output(

    AnswerWithCitations, include_raw=True

).with_config(run_name="GenerateAnswer")


from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper

from langchain_core.tools import tool



'''

# Tavily is typically a better search engine, but your free queries are limited

search_engine = TavilySearchResults(max_results=4)



@tool

async def search_engine(query: str):

    """Search engine to the internet."""

    results = tavily_search.invoke(query)

    return [{"content": r["content"], "url": r["url"]} for r in results]

'''



# DDG

search_engine = DuckDuckGoSearchAPIWrapper()





@tool

async def search_engine(query: str):

    """Search engine to the internet."""

    results = DuckDuckGoSearchAPIWrapper()._ddgs_text(query)

    return [{"content": r["body"], "url": r["href"]} for r in results]


import json



from langchain_core.runnables import RunnableConfig





async def gen_answer(

    state: InterviewState,

    config: Optional[RunnableConfig] = None,

    name: str = "Subject_Matter_Expert",

    max_str_len: int = 15000,

):

    swapped_state = swap_roles(state, name)  # Convert all other AI messages

    queries = await gen_queries_chain.ainvoke(swapped_state)

    query_results = await search_engine.abatch(

        queries["parsed"].queries, config, return_exceptions=True

    )

    successful_results = [

        res for res in query_results if not isinstance(res, Exception)

    ]

    all_query_results = {

        res["url"]: res["content"] for results in successful_results for res in results

    }

    # We could be more precise about handling max token length if we wanted to here

    dumped = json.dumps(all_query_results)[:max_str_len]

    ai_message: AIMessage = queries["raw"]

    tool_call = queries["raw"].tool_calls[0]

    tool_id = tool_call["id"]

    tool_message = ToolMessage(tool_call_id=tool_id, content=dumped)

    swapped_state["messages"].extend([ai_message, tool_message])

    # Only update the shared state with the final answer to avoid

    # polluting the dialogue history with intermediate messages

    generated = await gen_answer_chain.ainvoke(swapped_state)

    cited_urls = set(generated["parsed"].cited_urls)

    # Save the retrieved information to a the shared state for future reference

    cited_references = {k: v for k, v in all_query_results.items() if k in cited_urls}

    formatted_message = AIMessage(name=name, content=generated["parsed"].as_str)

    return {"messages": [formatted_message], "references": cited_references}


example_answer = await gen_answer(

    {"messages": [HumanMessage(content=question["messages"][0].content)]}

)

example_answer["messages"][-1].content


#### Construct the Interview Graph





Now that we've defined the editor and domain expert, we can compose them in a graph.


max_num_turns = 5

from langgraph.pregel import RetryPolicy





def route_messages(state: InterviewState, name: str = "Subject_Matter_Expert"):

    messages = state["messages"]

    num_responses = len(

        [m for m in messages if isinstance(m, AIMessage) and m.name == name]

    )

    if num_responses >= max_num_turns:

        return END

    last_question = messages[-2]

    if last_question.content.endswith("Thank you so much for your help!"):

        return END

    return "ask_question"





builder = StateGraph(InterviewState)



builder.add_node("ask_question", generate_question, retry=RetryPolicy(max_attempts=5))

builder.add_node("answer_question", gen_answer, retry=RetryPolicy(max_attempts=5))

builder.add_conditional_edges("answer_question", route_messages)

builder.add_edge("ask_question", "answer_question")



builder.add_edge(START, "ask_question")

interview_graph = builder.compile(checkpointer=False).with_config(

    run_name="Conduct Interviews"

)


from IPython.display import Image, display



try:

    display(Image(interview_graph.get_graph().draw_mermaid_png()))

except Exception:

    # This requires some extra dependencies and is optional

    pass


final_step = None



initial_state = {

    "editor": perspectives.editors[0],

    "messages": [

        AIMessage(

            content=f"So you said you were writing an article on {example_topic}?",

            name="Subject_Matter_Expert",

        )

    ],

}

async for step in interview_graph.astream(initial_state):

    name = next(iter(step))

    print(name)

    print("-- ", str(step[name]["messages"])[:300])

final_step = step


final_state = next(iter(final_step.values()))


## Refine Outline



At this point in STORM, we've conducted a large amount of research from different perspectives. It's time to refine the original outline based on these investigations. Below, create a chain using the LLM with a long context window to update the original outline.


refine_outline_prompt = ChatPromptTemplate.from_messages(

    [

        (

            "system",

            """You are a Wikipedia writer. You have gathered information from experts and search engines. Now, you are refining the outline of the Wikipedia page. \

You need to make sure that the outline is comprehensive and specific. \

Topic you are writing about: {topic} 



Old outline:



{old_outline}""",

        ),

        (

            "user",

            "Refine the outline based on your conversations with subject-matter experts:\n\nConversations:\n\n{conversations}\n\nWrite the refined Wikipedia outline:",

        ),

    ]

)



# Using turbo preview since the context can get quite long

refine_outline_chain = refine_outline_prompt | long_context_llm.with_structured_output(

    Outline

)


refined_outline = refine_outline_chain.invoke(

    {

        "topic": example_topic,

        "old_outline": initial_outline.as_str,

        "conversations": "\n\n".join(

            f"### {m.name}\n\n{m.content}" for m in final_state["messages"]

        ),

    }

)


print(refined_outline.as_str)


## Generate Article



Now it's time to generate the full article. We will first divide-and-conquer, so that each section can be tackled by an individual llm. Then we will prompt the long-form LLM to refine the finished article (since each section may use an inconsistent voice).



#### Create Retriever



The research process uncovers a large number of reference documents that we may want to query during the final article-writing process.



First, create the retriever:


from langchain_community.vectorstores import InMemoryVectorStore

from langchain_core.documents import Document

from langchain_openai import OpenAIEmbeddings



embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

reference_docs = [

    Document(page_content=v, metadata={"source": k})

    for k, v in final_state["references"].items()

]

# This really doesn't need to be a vectorstore for this size of data.

# It could just be a numpy matrix. Or you could store documents

# across requests if you want.

vectorstore = InMemoryVectorStore.from_documents(

    reference_docs,

    embedding=embeddings,

)

retriever = vectorstore.as_retriever(k=3)


retriever.invoke("What's a long context LLM anyway?")


#### Generate Sections



Now you can generate the sections using the indexed docs.


class SubSection(BaseModel):

    subsection_title: str = Field(..., title="Title of the subsection")

    content: str = Field(

        ...,

        title="Full content of the subsection. Include [#] citations to the cited sources where relevant.",

    )



    @property

    def as_str(self) -> str:

        return f"### {self.subsection_title}\n\n{self.content}".strip()





class WikiSection(BaseModel):

    section_title: str = Field(..., title="Title of the section")

    content: str = Field(..., title="Full content of the section")

    subsections: Optional[List[Subsection]] = Field(

        default=None,

        title="Titles and descriptions for each subsection of the Wikipedia page.",

    )

    citations: List[str] = Field(default_factory=list)



    @property

    def as_str(self) -> str:

        subsections = "\n\n".join(

            subsection.as_str for subsection in self.subsections or []

        )

        citations = "\n".join([f" [{i}] {cit}" for i, cit in enumerate(self.citations)])

        return (

            f"## {self.section_title}\n\n{self.content}\n\n{subsections}".strip()

            + f"\n\n{citations}".strip()

        )





section_writer_prompt = ChatPromptTemplate.from_messages(

    [

        (

            "system",

            "You are an expert Wikipedia writer. Complete your assigned WikiSection from the following outline:\n\n"

            "{outline}\n\nCite your sources, using the following references:\n\n<Documents>\n{docs}\n<Documents>",

        ),

        ("user", "Write the full WikiSection for the {section} section."),

    ]

)





async def retrieve(inputs: dict):

    docs = await retriever.ainvoke(inputs["topic"] + ": " + inputs["section"])

    formatted = "\n".join(

        [

            f'<Document href="{doc.metadata["source"]}"/>\n{doc.page_content}\n</Document>'

            for doc in docs

        ]

    )

    return {"docs": formatted, **inputs}





section_writer = (

    retrieve

    | section_writer_prompt

    | long_context_llm.with_structured_output(WikiSection)

)


section = await section_writer.ainvoke(

    {

        "outline": refined_outline.as_str,

        "section": refined_outline.sections[1].section_title,

        "topic": example_topic,

    }

)

print(section.as_str)


#### Generate final article



Now we can rewrite the draft to appropriately group all the citations and maintain a consistent voice.


from langchain_core.output_parsers import StrOutputParser



writer_prompt = ChatPromptTemplate.from_messages(

    [

        (

            "system",

            "You are an expert Wikipedia author. Write the complete wiki article on {topic} using the following section drafts:\n\n"

            "{draft}\n\nStrictly follow Wikipedia format guidelines.",

        ),

        (

            "user",

            'Write the complete Wiki article using markdown format. Organize citations using footnotes like "[1]",'

            " avoiding duplicates in the footer. Include URLs in the footer.",

        ),

    ]

)



writer = writer_prompt | long_context_llm | StrOutputParser()


for tok in writer.stream({"topic": example_topic, "draft": section.as_str}):

    print(tok, end="")


## Final Flow



Now it's time to string everything together. We will have 6 main stages in sequence:

.

1. Generate the initial outline + perspectives

2. Batch converse with each perspective to expand the content for the article

3. Refine the outline based on the conversations

4. Index the reference docs from the conversations

5. Write the individual sections of the article

6. Write the final wiki



The state tracks the outputs of each stage.


class ResearchState(TypedDict):

    topic: str

    outline: Outline

    editors: List[Editor]

    interview_results: List[InterviewState]

    # The final sections output

    sections: List[WikiSection]

    article: str


import asyncio





async def initialize_research(state: ResearchState):

    topic = state["topic"]

    coros = (

        generate_outline_direct.ainvoke({"topic": topic}),

        survey_subjects.ainvoke(topic),

    )

    results = await asyncio.gather(*coros)

    return {

        **state,

        "outline": results[0],

        "editors": results[1].editors,

    }





async def conduct_interviews(state: ResearchState):

    topic = state["topic"]

    initial_states = [

        {

            "editor": editor,

            "messages": [

                AIMessage(

                    content=f"So you said you were writing an article on {topic}?",

                    name="Subject_Matter_Expert",

                )

            ],

        }

        for editor in state["editors"]

    ]

    # We call in to the sub-graph here to parallelize the interviews

    interview_results = await interview_graph.abatch(initial_states)



    return {

        **state,

        "interview_results": interview_results,

    }





def format_conversation(interview_state):

    messages = interview_state["messages"]

    convo = "\n".join(f"{m.name}: {m.content}" for m in messages)

    return f'Conversation with {interview_state["editor"].name}\n\n' + convo





async def refine_outline(state: ResearchState):

    convos = "\n\n".join(

        [

            format_conversation(interview_state)

            for interview_state in state["interview_results"]

        ]

    )



    updated_outline = await refine_outline_chain.ainvoke(

        {

            "topic": state["topic"],

            "old_outline": state["outline"].as_str,

            "conversations": convos,

        }

    )

    return {**state, "outline": updated_outline}





async def index_references(state: ResearchState):

    all_docs = []

    for interview_state in state["interview_results"]:

        reference_docs = [

            Document(page_content=v, metadata={"source": k})

            for k, v in interview_state["references"].items()

        ]

        all_docs.extend(reference_docs)

    await vectorstore.aadd_documents(all_docs)

    return state





async def write_sections(state: ResearchState):

    outline = state["outline"]

    sections = await section_writer.abatch(

        [

            {

                "outline": refined_outline.as_str,

                "section": section.section_title,

                "topic": state["topic"],

            }

            for section in outline.sections

        ]

    )

    return {

        **state,

        "sections": sections,

    }





async def write_article(state: ResearchState):

    topic = state["topic"]

    sections = state["sections"]

    draft = "\n\n".join([section.as_str for section in sections])

    article = await writer.ainvoke({"topic": topic, "draft": draft})

    return {

        **state,

        "article": article,

    }


#### Create the graph


from langgraph.checkpoint.memory import MemorySaver



builder_of_storm = StateGraph(ResearchState)



nodes = [

    ("init_research", initialize_research),

    ("conduct_interviews", conduct_interviews),

    ("refine_outline", refine_outline),

    ("index_references", index_references),

    ("write_sections", write_sections),

    ("write_article", write_article),

]

for i in range(len(nodes)):

    name, node = nodes[i]

    builder_of_storm.add_node(name, node, retry=RetryPolicy(max_attempts=3))

    if i > 0:

        builder_of_storm.add_edge(nodes[i - 1][0], name)



builder_of_storm.add_edge(START, nodes[0][0])

builder_of_storm.add_edge(nodes[-1][0], END)

storm = builder_of_storm.compile(checkpointer=MemorySaver())


from IPython.display import Image, display



try:

    display(Image(storm.get_graph().draw_mermaid_png()))

except Exception:

    # This requires some extra dependencies and is optional

    pass


config = {"configurable": {"thread_id": "my-thread"}}

async for step in storm.astream(

    {

        "topic": "Groq, NVIDIA, Llamma.cpp and the future of LLM Inference",

    },

    config,

):

    name = next(iter(step))

    print(name)

    print("-- ", str(step[name])[:300])


checkpoint = storm.get_state(config)

article = checkpoint.values["article"]


## Render the Wiki



Now we can render the final wiki page!


from IPython.display import Markdown



# We will down-header the sections to create less confusion in this notebook

Markdown(article.replace("\n#", "\n##"))










##### FILE: self-discover.ipynb #####


# Self-Discover Agent



An implementation of the [Self-Discover paper](https://arxiv.org/pdf/2402.03620.pdf).



Based on [this implementation from @catid](https://github.com/catid/self-discover/tree/main?tab=readme-ov-file)





## Setup



First, let's install our required packages and set our API keys


%%capture --no-stderr

%pip install -U --quiet langchain langgraph langchain_openai


import getpass

import os





def _set_if_undefined(var: str) -> None:

    if os.environ.get(var):

        return

    os.environ[var] = getpass.getpass(var)





_set_if_undefined("OPENAI_API_KEY")


<div class="admonition tip">

    <p class="admonition-title">Set up <a href="https://smith.langchain.com">LangSmith</a> for LangGraph development</p>

    <p style="padding-top: 5px;">

        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph  read more about how to get started <a href="https://docs.smith.langchain.com">here</a>. 

    </p>

</div>   


## Define the prompts


from langchain import hub



select_prompt = hub.pull("hwchase17/self-discovery-select")

print("Self-Discovery Select Prompt:")

select_prompt.pretty_print()

print("Self-Discovery Select Response:")

adapt_prompt = hub.pull("hwchase17/self-discovery-adapt")

adapt_prompt.pretty_print()

structured_prompt = hub.pull("hwchase17/self-discovery-structure")

print("Self-Discovery Structured Prompt:")

structured_prompt.pretty_print()

reasoning_prompt = hub.pull("hwchase17/self-discovery-reasoning")

print("Self-Discovery Structured Response:")

reasoning_prompt.pretty_print()


## Define the graph


from typing import Optional

from typing_extensions import TypedDict



from langchain_core.output_parsers import StrOutputParser

from langchain_openai import ChatOpenAI



from langgraph.graph import END, START, StateGraph





class SelfDiscoverState(TypedDict):

    reasoning_modules: str

    task_description: str

    selected_modules: Optional[str]

    adapted_modules: Optional[str]

    reasoning_structure: Optional[str]

    answer: Optional[str]





model = ChatOpenAI(temperature=0, model="gpt-4-turbo-preview")





def select(inputs):

    select_chain = select_prompt | model | StrOutputParser()

    return {"selected_modules": select_chain.invoke(inputs)}





def adapt(inputs):

    adapt_chain = adapt_prompt | model | StrOutputParser()

    return {"adapted_modules": adapt_chain.invoke(inputs)}





def structure(inputs):

    structure_chain = structured_prompt | model | StrOutputParser()

    return {"reasoning_structure": structure_chain.invoke(inputs)}





def reason(inputs):

    reasoning_chain = reasoning_prompt | model | StrOutputParser()

    return {"answer": reasoning_chain.invoke(inputs)}





graph = StateGraph(SelfDiscoverState)

graph.add_node(select)

graph.add_node(adapt)

graph.add_node(structure)

graph.add_node(reason)

graph.add_edge(START, "select")

graph.add_edge("select", "adapt")

graph.add_edge("adapt", "structure")

graph.add_edge("structure", "reason")

graph.add_edge("reason", END)

app = graph.compile()


## Invoke the graph


reasoning_modules = [

    "1. How could I devise an experiment to help solve that problem?",

    "2. Make a list of ideas for solving this problem, and apply them one by one to the problem to see if any progress can be made.",

    # "3. How could I measure progress on this problem?",

    "4. How can I simplify the problem so that it is easier to solve?",

    "5. What are the key assumptions underlying this problem?",

    "6. What are the potential risks and drawbacks of each solution?",

    "7. What are the alternative perspectives or viewpoints on this problem?",

    "8. What are the long-term implications of this problem and its solutions?",

    "9. How can I break down this problem into smaller, more manageable parts?",

    "10. Critical Thinking: This style involves analyzing the problem from different perspectives, questioning assumptions, and evaluating the evidence or information available. It focuses on logical reasoning, evidence-based decision-making, and identifying potential biases or flaws in thinking.",

    "11. Try creative thinking, generate innovative and out-of-the-box ideas to solve the problem. Explore unconventional solutions, thinking beyond traditional boundaries, and encouraging imagination and originality.",

    # "12. Seek input and collaboration from others to solve the problem. Emphasize teamwork, open communication, and leveraging the diverse perspectives and expertise of a group to come up with effective solutions.",

    "13. Use systems thinking: Consider the problem as part of a larger system and understanding the interconnectedness of various elements. Focuses on identifying the underlying causes, feedback loops, and interdependencies that influence the problem, and developing holistic solutions that address the system as a whole.",

    "14. Use Risk Analysis: Evaluate potential risks, uncertainties, and tradeoffs associated with different solutions or approaches to a problem. Emphasize assessing the potential consequences and likelihood of success or failure, and making informed decisions based on a balanced analysis of risks and benefits.",

    # "15. Use Reflective Thinking: Step back from the problem, take the time for introspection and self-reflection. Examine personal biases, assumptions, and mental models that may influence problem-solving, and being open to learning from past experiences to improve future approaches.",

    "16. What is the core issue or problem that needs to be addressed?",

    "17. What are the underlying causes or factors contributing to the problem?",

    "18. Are there any potential solutions or strategies that have been tried before? If yes, what were the outcomes and lessons learned?",

    "19. What are the potential obstacles or challenges that might arise in solving this problem?",

    "20. Are there any relevant data or information that can provide insights into the problem? If yes, what data sources are available, and how can they be analyzed?",

    "21. Are there any stakeholders or individuals who are directly affected by the problem? What are their perspectives and needs?",

    "22. What resources (financial, human, technological, etc.) are needed to tackle the problem effectively?",

    "23. How can progress or success in solving the problem be measured or evaluated?",

    "24. What indicators or metrics can be used?",

    "25. Is the problem a technical or practical one that requires a specific expertise or skill set? Or is it more of a conceptual or theoretical problem?",

    "26. Does the problem involve a physical constraint, such as limited resources, infrastructure, or space?",

    "27. Is the problem related to human behavior, such as a social, cultural, or psychological issue?",

    "28. Does the problem involve decision-making or planning, where choices need to be made under uncertainty or with competing objectives?",

    "29. Is the problem an analytical one that requires data analysis, modeling, or optimization techniques?",

    "30. Is the problem a design challenge that requires creative solutions and innovation?",

    "31. Does the problem require addressing systemic or structural issues rather than just individual instances?",

    "32. Is the problem time-sensitive or urgent, requiring immediate attention and action?",

    "33. What kinds of solution typically are produced for this kind of problem specification?",

    "34. Given the problem specification and the current best solution, have a guess about other possible solutions."

    "35. Lets imagine the current best solution is totally wrong, what other ways are there to think about the problem specification?"

    "36. What is the best way to modify this current best solution, given what you know about these kinds of problem specification?"

    "37. Ignoring the current best solution, create an entirely new solution to the problem."

    # "38. Lets think step by step."

    "39. Lets make a step by step plan and implement it with good notation and explanation.",

]





task_example = "Lisa has 10 apples. She gives 3 apples to her friend and then buys 5 more apples from the store. How many apples does Lisa have now?"



task_example = """This SVG path element <path d="M 55.57,80.69 L 57.38,65.80 M 57.38,65.80 L 48.90,57.46 M 48.90,57.46 L

45.58,47.78 M 45.58,47.78 L 53.25,36.07 L 66.29,48.90 L 78.69,61.09 L 55.57,80.69"/> draws a:

(A) circle (B) heptagon (C) hexagon (D) kite (E) line (F) octagon (G) pentagon(H) rectangle (I) sector (J) triangle"""



reasoning_modules_str = "\n".join(reasoning_modules)



for s in app.stream(

    {"task_description": task_example, "reasoning_modules": reasoning_modules_str}

):

    print(s)







